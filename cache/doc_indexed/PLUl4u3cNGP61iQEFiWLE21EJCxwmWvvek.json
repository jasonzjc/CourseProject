{"Intro: A New Way to Start Linear Algebra": [{"text": "GILBERT STRANG: Well, I'm Gil Strang.", "start": 13.5, "duration": 2.22}, {"text": "And I'm very happy if you know the linear algebra", "start": 15.72, "duration": 4.93}, {"text": "videos on OpenCourseWare or on YouTube.", "start": 20.65, "duration": 3.97}, {"text": "That's for the math course 18.06.", "start": 24.62, "duration": 4.13}, {"text": "And I'm even happier if you like them.", "start": 28.75, "duration": 3.659}, {"text": "And I'm here today to update them for several reasons.", "start": 32.409, "duration": 6.181}, {"text": "Well, a lots happened in linear algebra in these years.", "start": 38.59, "duration": 6.73}, {"text": "Fortunately, the whole subject has just", "start": 45.32, "duration": 2.96}, {"text": "become more and more essential, more and more important, more", "start": 48.28, "duration": 3.48}, {"text": "and more beautiful.", "start": 51.76, "duration": 1.62}, {"text": "And so I wanted to say something about those later steps.", "start": 53.38, "duration": 6.27}, {"text": "And also, when I teach it now, I have a new starting point.", "start": 59.65, "duration": 6.06}, {"text": "And I'll show you that.", "start": 65.71, "duration": 1.27}, {"text": "So I'll go a little slowly on that starting point.", "start": 66.98, "duration": 3.13}, {"text": "The slides tell you the whole course.", "start": 70.11, "duration": 2.35}, {"text": "And that's crazy to have a full course within a short video.", "start": 72.46, "duration": 5.88}, {"text": "But especially the first part is new.", "start": 78.34, "duration": 5.31}, {"text": "And I'm even writing a textbook called", "start": 83.65, "duration": 3.93}, {"text": "Linear Algebra for Everyone that will start this way.", "start": 87.58, "duration": 3.73}, {"text": "I hope that the new start brings in real linear algebra ideas", "start": 91.31, "duration": 8.66}, {"text": "right away.", "start": 99.97, "duration": 1.06}, {"text": "And let me show you where those are.", "start": 101.03, "duration": 1.67}, {"text": "So this is an outline of the whole video.", "start": 102.7, "duration": 5.1}, {"text": "And the first line, which I think", "start": 107.8, "duration": 3.84}, {"text": "about in my mind as matrix A, is a product", "start": 111.64, "duration": 4.29}, {"text": "of a C matrix, a column matrix, and a row matrix R.", "start": 115.93, "duration": 4.8}, {"text": "And you'll see what those are.", "start": 120.73, "duration": 1.87}, {"text": "So that's the new idea.", "start": 122.6, "duration": 3.78}, {"text": "That will come first.", "start": 126.38, "duration": 1.7}, {"text": "Then these are five famous essential shorthand", "start": 128.08, "duration": 7.35}, {"text": "descriptions of the key chapters of linear algebra,", "start": 135.43, "duration": 4.29}, {"text": "the key chapters.", "start": 139.72, "duration": 0.99}, {"text": "So they represent, for example, LU is those letters are famous.", "start": 140.71, "duration": 5.01}, {"text": "And computer commands would be exactly", "start": 145.72, "duration": 3.21}, {"text": "those letters LU for elimination for solving equations,", "start": 148.93, "duration": 5.5}, {"text": "the first job of linear algebra.", "start": 154.43, "duration": 2.28}, {"text": "And then QR.", "start": 156.71, "duration": 2.69}, {"text": "So Q is a very interesting important type of matrix.", "start": 159.4, "duration": 4.26}, {"text": "That's standing for an orthogonal matrix.", "start": 163.66, "duration": 2.49}, {"text": "There is the word orthogonal or perpendicular.", "start": 166.15, "duration": 4.5}, {"text": "So those are the best matrices to compute with.", "start": 170.65, "duration": 3.69}, {"text": "And that QR gets us there.", "start": 174.34, "duration": 3.91}, {"text": "And S is for a symmetric matrix.", "start": 178.25, "duration": 2.39}, {"text": "And it will involve--", "start": 180.64, "duration": 1.46}, {"text": "oh, well, I should say, the first half,", "start": 182.1, "duration": 2.47}, {"text": "ending with there, with QR, is about solving equations.", "start": 184.57, "duration": 7.08}, {"text": "The second half, these three are about eigenvalues", "start": 191.65, "duration": 4.62}, {"text": "and eigenvectors and singular values,", "start": 196.27, "duration": 2.19}, {"text": "a different way to approach the whole subject and a very, very", "start": 198.46, "duration": 4.59}, {"text": "important way.", "start": 203.05, "duration": 1.71}, {"text": "Among my goals is to help courses around the world", "start": 204.76, "duration": 5.55}, {"text": "get singular values included because you really", "start": 210.31, "duration": 4.41}, {"text": "don't want to miss them.", "start": 214.72, "duration": 1.74}, {"text": "That's the high point of the theory.", "start": 216.46, "duration": 2.88}, {"text": "And it's expressed like all the others", "start": 219.34, "duration": 3.4}, {"text": "as breaking a matrix into two or three pieces, two", "start": 222.74, "duration": 5.84}, {"text": "or three parts.", "start": 228.58, "duration": 1.35}, {"text": "So that's my plan for this video.", "start": 229.93, "duration": 4.2}, {"text": "And I hope it's helpful.", "start": 234.13, "duration": 2.01}, {"text": "Again, it's a whole course in a short time.", "start": 236.14, "duration": 4.18}, {"text": "And please go to the real 18.06 videos for the details.", "start": 240.32, "duration": 7.92}, {"text": "Thanks", "start": 248.24, "duration": 1.25}], "Part 1: The Column Space of a Matrix": [{"text": "GILBERT STRANG: OK, here's the, well, the title slide.", "start": 16.82, "duration": 3.65}, {"text": "Since this year happened to be 2020,", "start": 20.47, "duration": 2.28}, {"text": "and that means clear vision, I thought", "start": 22.75, "duration": 2.28}, {"text": "I'd get that into the title of these slides.", "start": 25.03, "duration": 5.4}, {"text": "And then you've seen in these six pieces as a sort of look", "start": 30.43, "duration": 9.45}, {"text": "ahead, and I'm going to start on that first piece, A equals CR.", "start": 39.88, "duration": 5.61}, {"text": "That's the new way I like to start teaching linear algebra.", "start": 45.49, "duration": 4.71}, {"text": "And I'll tell you why.", "start": 50.2, "duration": 1.79}, {"text": "OK, oh, here, we have a few examples.", "start": 51.99, "duration": 2.65}, {"text": "Well, that will lead to our ideas.", "start": 54.64, "duration": 2.7}, {"text": "You see that matrix, A0.", "start": 57.34, "duration": 2.53}, {"text": "A matrix is just a square or a rectangle of numbers.", "start": 59.87, "duration": 4.61}, {"text": "But those numbers have special features.", "start": 64.48, "duration": 4.44}, {"text": "If you look closely, well, you say 1, 3, 2 as row 1.", "start": 68.92, "duration": 5.19}, {"text": "And then what do you see for row 3?", "start": 74.11, "duration": 2.7}, {"text": "2, 6, 4.", "start": 76.81, "duration": 1.32}, {"text": "And those are two vectors in the same direction.", "start": 78.13, "duration": 5.22}, {"text": "Why is that?", "start": 83.35, "duration": 0.66}, {"text": "Because 2, 6, 4 is exactly 2 times 1, 3, 2.", "start": 84.01, "duration": 4.94}, {"text": "And in the middle there is 4 times 1, 3, 2.", "start": 88.95, "duration": 3.13}, {"text": "So I have three rows in the same direction.", "start": 92.08, "duration": 3.78}, {"text": "And actually, also, this is the magic.", "start": 95.86, "duration": 2.55}, {"text": "Can I tell you this right at the start?", "start": 98.41, "duration": 2.295}, {"text": "The columns, look at the columns.", "start": 100.705, "duration": 2.685}, {"text": "1, 4, 2.", "start": 103.39, "duration": 1.56}, {"text": "If I multiply that by 3, I get 3, 12, 6.", "start": 104.95, "duration": 3.33}, {"text": "If I multiply it by 2, I get 2, 8, 4.", "start": 108.28, "duration": 2.51}, {"text": "So somehow, magically, the columns", "start": 110.79, "duration": 3.82}, {"text": "are in the same direction exactly when the rows", "start": 114.61, "duration": 3.18}, {"text": "are in the same direction.", "start": 117.79, "duration": 1.45}, {"text": "They're different.", "start": 119.24, "duration": 0.8}, {"text": "That's what linear algebra is about,", "start": 120.04, "duration": 2.1}, {"text": "the relations between columns and rows.", "start": 122.14, "duration": 3.44}, {"text": "OK, and well, here's another one I'll look at.", "start": 125.58, "duration": 4.69}, {"text": "There again, you see row 1 plus row 2 equal row 3.", "start": 130.27, "duration": 5.53}, {"text": "So it's not quite like this where every row", "start": 135.8, "duration": 4.17}, {"text": "was in the same direction.", "start": 139.97, "duration": 2.76}, {"text": "But here is if I add rows 1 and 2, I get row 2.", "start": 142.73, "duration": 4.59}, {"text": "So that's a matrix of rank 2, we'll say.", "start": 147.32, "duration": 3.51}, {"text": "You'll see it.", "start": 150.83, "duration": 1.02}, {"text": "OK, then here here, S is for symmetric matrices.", "start": 151.85, "duration": 4.8}, {"text": "Those are the kings of linear algebra.", "start": 156.65, "duration": 1.8}, {"text": "And here are a few small samples.", "start": 158.45, "duration": 3.42}, {"text": "And the queens of linear algebra are", "start": 161.87, "duration": 3.0}, {"text": "these matrices I call Q. Those are called orthogonal matrices.", "start": 164.87, "duration": 6.23}, {"text": "Orthogonal meaning perpendicular.", "start": 171.1, "duration": 2.47}, {"text": "So and they tend to express a rotation.", "start": 173.57, "duration": 6.52}, {"text": "So that's a rotation matrix, an orthogonal matrix.", "start": 180.09, "duration": 3.26}, {"text": "That rotates the plane.", "start": 183.35, "duration": 2.22}, {"text": "And there is a pretty general matrix", "start": 185.57, "duration": 2.67}, {"text": "that we'll see at the very end.", "start": 188.24, "duration": 1.755}, {"text": "OK, so I'm into the start of the column space.", "start": 189.995, "duration": 4.765}, {"text": "So that's a word I don't use in the videos for quite a while.", "start": 194.76, "duration": 4.64}, {"text": "But here, you see I'm using it in the first minutes.", "start": 199.4, "duration": 3.06}, {"text": "So I look at a matrix.", "start": 202.46, "duration": 1.9}, {"text": "Well, first, let's just remember how", "start": 204.36, "duration": 2.18}, {"text": "to multiply a matrix by a vector.", "start": 206.54, "duration": 2.9}, {"text": "OK, there is a matrix A. There is a vector", "start": 209.44, "duration": 3.13}, {"text": "x with three components.", "start": 212.57, "duration": 2.01}, {"text": "And the way I like to multiply them", "start": 214.58, "duration": 2.58}, {"text": "is to take the columns of A. That's what I'm focusing on,", "start": 217.16, "duration": 3.33}, {"text": "columns of A. There they are, 1, 2, and 3.", "start": 220.49, "duration": 3.72}, {"text": "I multiply them by those three numbers x1, x2, x3, and I add.", "start": 224.21, "duration": 5.1}, {"text": "And that's called a linear combination.", "start": 229.31, "duration": 2.97}, {"text": "Linear because nothing is squared or cubed or anything.", "start": 232.28, "duration": 4.02}, {"text": "And combination because I'm putting them together,", "start": 236.3, "duration": 3.54}, {"text": "adding them together.", "start": 239.84, "duration": 1.59}, {"text": "OK, so that's the idea.", "start": 241.43, "duration": 2.37}, {"text": "And now, the big idea is in that top line.", "start": 243.8, "duration": 4.68}, {"text": "I want to think of all combinations.", "start": 248.48, "duration": 2.8}, {"text": "So this is one particular combination", "start": 251.28, "duration": 2.87}, {"text": "with a particular x1 and x2 and x3.", "start": 254.15, "duration": 3.61}, {"text": "But now, I think of every x1 and x2 and x3,", "start": 257.76, "duration": 4.85}, {"text": "all the vectors that I could get.", "start": 262.61, "duration": 2.55}, {"text": "Well, of course, I could get the first column", "start": 265.16, "duration": 2.73}, {"text": "by taking 1 and 0 and 0.", "start": 267.89, "duration": 2.52}, {"text": "That would give me the first column.", "start": 270.41, "duration": 1.71}, {"text": "But it's really mixtures of the columns that this produces.", "start": 272.12, "duration": 4.41}, {"text": "And it fills out.", "start": 276.53, "duration": 2.16}, {"text": "It fills out, in this case, a whole plane", "start": 278.69, "duration": 3.15}, {"text": "in three dimensions.", "start": 281.84, "duration": 1.5}, {"text": "These vectors have three components.", "start": 283.34, "duration": 1.82}, {"text": "We're in three dimensions.", "start": 285.16, "duration": 1.75}, {"text": "And can you just imagine in your head,", "start": 286.91, "duration": 3.57}, {"text": "two lines meeting at 0, 0, 0.", "start": 290.48, "duration": 5.13}, {"text": "So they cross.", "start": 295.61, "duration": 2.07}, {"text": "But I just have two lines.", "start": 297.68, "duration": 1.5}, {"text": "And now, I fill in between those lines.", "start": 299.18, "duration": 4.92}, {"text": "Filling in between those two lines", "start": 304.1, "duration": 2.58}, {"text": "is taking the linear combinations.", "start": 306.68, "duration": 2.75}, {"text": "That's where they are.", "start": 309.43, "duration": 1.39}, {"text": "And the result is I get a plane.", "start": 310.82, "duration": 3.45}, {"text": "I do not get the whole space because nothing", "start": 314.27, "duration": 2.31}, {"text": "is going in a third direction for this matrix.", "start": 316.58, "duration": 4.14}, {"text": "All right.", "start": 320.72, "duration": 1.14}, {"text": "So let's see more about this.", "start": 321.86, "duration": 2.37}, {"text": "So that's that word column space.", "start": 324.23, "duration": 2.73}, {"text": "And I use the capital C for that.", "start": 326.96, "duration": 2.73}, {"text": "And it's all the vectors I can get", "start": 329.69, "duration": 2.61}, {"text": "that way, all the combinations of the columns.", "start": 332.3, "duration": 3.31}, {"text": "And now I ask.", "start": 335.61, "duration": 1.04}, {"text": "Oh, well, maybe I just answered this question.", "start": 336.65, "duration": 2.7}, {"text": "Sorry.", "start": 339.35, "duration": 1.05}, {"text": "I ask, is the column space, all the combinations,", "start": 340.4, "duration": 4.38}, {"text": "is it the whole 3D space, which everybody calls R3 for real 3,", "start": 344.78, "duration": 7.92}, {"text": "or is it a plane, or is it just a line?", "start": 352.7, "duration": 3.0}, {"text": "Well, the answer is plane.", "start": 355.7, "duration": 2.55}, {"text": "That probably even gives us the answer.", "start": 358.25, "duration": 2.46}, {"text": "That's the good thing about this subject.", "start": 360.71, "duration": 1.92}, {"text": "The answer is a plane because I have two different lines that", "start": 362.63, "duration": 4.71}, {"text": "meet at the 0.", "start": 367.34, "duration": 2.07}, {"text": "And when I fill in between them, I have a flat plane.", "start": 369.41, "duration": 3.48}, {"text": "I don't go in the third direction.", "start": 372.89, "duration": 2.1}, {"text": "Good.", "start": 374.99, "duration": 0.9}, {"text": "So that's the column space for this matrix.", "start": 375.89, "duration": 5.1}, {"text": "And here's a little more saying about that.", "start": 380.99, "duration": 2.97}, {"text": "We kept column 1.", "start": 383.96, "duration": 2.8}, {"text": "And we kept column 2 because you remember", "start": 386.76, "duration": 3.27}, {"text": "those two columns, the first two, were different.", "start": 390.03, "duration": 3.173}, {"text": "They went in different directions.", "start": 393.203, "duration": 1.417}, {"text": "They go in different directions.", "start": 394.62, "duration": 1.96}, {"text": "We did not keep the third column because it was just", "start": 396.58, "duration": 3.5}, {"text": "the sum of the first two.", "start": 400.08, "duration": 1.23}, {"text": "It's on the plane, nothing new.", "start": 401.31, "duration": 3.45}, {"text": "So the real meat of the matrix A is in the column matrix C", "start": 404.76, "duration": 5.13}, {"text": "that has just the two columns.", "start": 409.89, "duration": 2.52}, {"text": "And what about R?", "start": 412.41, "duration": 1.29}, {"text": "Because this is my plan for the first few weeks,", "start": 413.7, "duration": 5.28}, {"text": "first two to three weeks of linear algebra,", "start": 418.98, "duration": 3.06}, {"text": "is to understand.", "start": 422.04, "duration": 3.15}, {"text": "So that 5, 5, 3 would be called a dependent vector because it", "start": 425.19, "duration": 4.74}, {"text": "depends on the first two.", "start": 429.93, "duration": 2.46}, {"text": "Those were independent.", "start": 432.39, "duration": 2.34}, {"text": "So those are the two that I keep in the matrix C.", "start": 434.73, "duration": 5.46}, {"text": "And then that matrix R, oh, well, now I'm", "start": 440.19, "duration": 3.75}, {"text": "multiplying two matrices.", "start": 443.94, "duration": 1.56}, {"text": "And you know how to do that.", "start": 445.5, "duration": 1.35}, {"text": "But I always have another way to look at it.", "start": 446.85, "duration": 4.03}, {"text": "So the way I look at it is by linear combinations.", "start": 450.88, "duration": 3.275}, {"text": "Do you remember those?", "start": 454.155, "duration": 1.725}, {"text": "So multiplying is a combination of these guys.", "start": 455.88, "duration": 4.26}, {"text": "First, I have 1 of the first column.", "start": 460.14, "duration": 3.03}, {"text": "That's my first column.", "start": 463.17, "duration": 1.7}, {"text": "And the next time, I have 1 of the second column.", "start": 464.87, "duration": 3.01}, {"text": "That's my second vector.", "start": 467.88, "duration": 1.59}, {"text": "And the third one is this guy, 1 of that and 1 of that.", "start": 469.47, "duration": 5.17}, {"text": "So these two are the independent ones, and that's dependent.", "start": 474.64, "duration": 3.62}, {"text": "And a full set of independent ones", "start": 478.26, "duration": 2.67}, {"text": "is called a basis, really fundamental.", "start": 480.93, "duration": 3.27}, {"text": "So I guess I think that linear algebra should just", "start": 484.2, "duration": 2.85}, {"text": "start with these key ideas, just go with them.", "start": 487.05, "duration": 4.53}, {"text": "And we learned something.", "start": 491.58, "duration": 2.07}, {"text": "It almost falls in our laps.", "start": 493.65, "duration": 1.6}, {"text": "It's a first great and not obvious", "start": 495.25, "duration": 3.62}, {"text": "fact about linear algebra.", "start": 498.87, "duration": 2.82}, {"text": "I'm just amazed to have it here.", "start": 501.69, "duration": 4.0}, {"text": "The number of independent columns in A, which it was two,", "start": 505.69, "duration": 7.09}, {"text": "is equal to the number of independent rows in R, also", "start": 512.78, "duration": 4.56}, {"text": "two.", "start": 517.34, "duration": 0.64}, {"text": "You remember that we had two rows and two columns?", "start": 517.98, "duration": 3.59}, {"text": "So two columns first in C, two rows in R. And the point", "start": 521.57, "duration": 5.61}, {"text": "is that that's telling us--", "start": 527.18, "duration": 3.01}, {"text": "and we just checked that those two rows were--", "start": 530.19, "duration": 2.99}, {"text": "two columns were independent.", "start": 533.18, "duration": 1.52}, {"text": "The two rows are independent.", "start": 534.7, "duration": 2.8}, {"text": "The basis, and then we learned that the column space", "start": 537.5, "duration": 7.19}, {"text": "has dimension 2.", "start": 544.69, "duration": 1.5}, {"text": "R equals 2 for this example.", "start": 546.19, "duration": 2.56}, {"text": "And the row space has the same dimension.", "start": 548.75, "duration": 3.63}, {"text": "So that column rank R equals the row", "start": 552.38, "duration": 3.23}, {"text": "rank R. It's like if you had a 50 by 80 matrix,", "start": 555.61, "duration": 6.37}, {"text": "OK, that's 4,000 numbers.", "start": 561.98, "duration": 2.51}, {"text": "You couldn't see what those these dimensions are.", "start": 564.49, "duration": 2.76}, {"text": "But linear algebra is telling you", "start": 567.25, "duration": 1.95}, {"text": "that a dimension of the row space and the column space,", "start": 569.2, "duration": 3.6}, {"text": "50 of one and 80 in another, are equal.", "start": 572.8, "duration": 4.42}, {"text": "OK, so this is again coming early, and we'll see it again.", "start": 577.22, "duration": 5.41}, {"text": "But it's good to start linear algebra from day one.", "start": 582.63, "duration": 5.47}, {"text": "And then here is another great fact about equations", "start": 588.1, "duration": 4.53}, {"text": "because matrices lead to these two equations", "start": 592.63, "duration": 4.33}, {"text": "where x is the unknown.", "start": 596.96, "duration": 1.57}, {"text": "And this equation has 0 on the right hand side.", "start": 598.53, "duration": 4.02}, {"text": "So how could we get 0 on the right hand side?", "start": 602.55, "duration": 4.34}, {"text": "We could take 1 of that.", "start": 606.89, "duration": 1.38}, {"text": "And let me change that to a minus sign and that to a minus", "start": 608.27, "duration": 2.86}, {"text": "Sign.", "start": 611.13, "duration": 0.68}, {"text": "One of those minus one of those minus one of those", "start": 611.81, "duration": 3.27}, {"text": "would be 0, 0, 0.", "start": 615.08, "duration": 2.46}, {"text": "So that 1 and minus 1 and minus 1 would tell us an x.", "start": 617.54, "duration": 5.59}, {"text": "And that's the solution.", "start": 623.13, "duration": 2.975}, {"text": "In applying linear algebra in engineering,", "start": 626.105, "duration": 2.515}, {"text": "in physics, in economics, in business,", "start": 628.62, "duration": 5.8}, {"text": "you end up with equations.", "start": 634.42, "duration": 1.46}, {"text": "Things balance.", "start": 635.88, "duration": 1.59}, {"text": "And you want to know how many solutions there are.", "start": 637.47, "duration": 3.75}, {"text": "And linear algebra was created to answer that question.", "start": 641.22, "duration": 3.57}, {"text": "OK, so now, I'm just going to say a little more", "start": 644.79, "duration": 2.49}, {"text": "about this starting method of the course.", "start": 647.28, "duration": 4.02}, {"text": "Oh, I want to focus here on these interesting matrices,", "start": 651.3, "duration": 6.54}, {"text": "where every column is a multiple of the first column.", "start": 657.84, "duration": 4.87}, {"text": "Every row is a multiple of the first row.", "start": 662.71, "duration": 4.02}, {"text": "Instead of having two independent columns and rows,", "start": 666.73, "duration": 3.27}, {"text": "these matrices have only one.", "start": 670.0, "duration": 2.13}, {"text": "So then C has one column.", "start": 672.13, "duration": 3.15}, {"text": "And R has one row.", "start": 675.28, "duration": 2.91}, {"text": "And the rank is 1.", "start": 678.19, "duration": 1.89}, {"text": "These are the building blocks of linear algebra, these rank 1", "start": 680.08, "duration": 5.31}, {"text": "matrices, column times row.", "start": 685.39, "duration": 3.74}, {"text": "The previous matrix would have one of those blocks", "start": 689.13, "duration": 3.61}, {"text": "and a second block.", "start": 692.74, "duration": 1.67}, {"text": "A big matrix from data science would have hundreds of blocks.", "start": 694.41, "duration": 4.79}, {"text": "But the great theorem in linear algebra", "start": 699.2, "duration": 3.68}, {"text": "is to break that big matrix into these simple pieces.", "start": 702.88, "duration": 4.41}, {"text": "So that's the goal for the end of the course.", "start": 707.29, "duration": 3.72}, {"text": "OK, and finally, a last thought about these.", "start": 711.01, "duration": 3.51}, {"text": "So this is C times R. I'm urging teachers", "start": 714.52, "duration": 3.96}, {"text": "to present that part at the early.", "start": 718.48, "duration": 4.47}, {"text": "So what are the good things, I've marked with a plus.", "start": 722.95, "duration": 3.78}, {"text": "First of all, the columns, we're looking at them in C.", "start": 726.73, "duration": 5.46}, {"text": "And we see them from A. We take them directly from A.", "start": 732.19, "duration": 3.512}, {"text": "R turns out to be a famous matrix.", "start": 735.702, "duration": 4.488}, {"text": "Row reduced echelon form it's called.", "start": 740.19, "duration": 3.42}, {"text": "So to see that pop up here is terrific.", "start": 743.61, "duration": 4.32}, {"text": "And then this wonderful fact that row", "start": 747.93, "duration": 2.52}, {"text": "rank equal column rank is clear from this C times R.", "start": 750.45, "duration": 5.82}, {"text": "So those are all terrifically good things.", "start": 756.27, "duration": 2.265}, {"text": "The other thing I have to say is that C and R are not", "start": 761.23, "duration": 4.1}, {"text": "great for avoiding round off or being", "start": 765.33, "duration": 5.0}, {"text": "good in large computations.", "start": 770.33, "duration": 2.64}, {"text": "This is a first factorization but not", "start": 772.97, "duration": 4.14}, {"text": "the best one for big computing.", "start": 777.11, "duration": 4.87}, {"text": "Right.", "start": 781.98, "duration": 0.65}, {"text": "So ill conditioned means they are difficult to deal with.", "start": 782.63, "duration": 5.37}, {"text": "And also, we often have a matrix with all the columns", "start": 788.0, "duration": 6.48}, {"text": "are independent.", "start": 794.48, "duration": 1.23}, {"text": "And it's a square matrix.", "start": 795.71, "duration": 2.07}, {"text": "All the columns are independent.", "start": 797.78, "duration": 1.45}, {"text": "We can solve Ax equals b all the time.", "start": 799.23, "duration": 2.66}, {"text": "But then if all the columns are independent,", "start": 801.89, "duration": 3.07}, {"text": "then our matrix C is just the same as A.", "start": 804.96, "duration": 2.48}, {"text": "We didn't get anywhere.", "start": 807.44, "duration": 1.65}, {"text": "And R would be the identity matrix, like a 1,", "start": 809.09, "duration": 3.27}, {"text": "because A equals C. So this is the starting point,", "start": 812.36, "duration": 7.58}, {"text": "picking out the independent columns, but not the end,", "start": 819.94, "duration": 3.38}, {"text": "of course.", "start": 823.32, "duration": 1.06}, {"text": "And I'll stop here and pick up on the next factorization", "start": 824.38, "duration": 7.76}, {"text": "right away.", "start": 832.14, "duration": 0.81}, {"text": "Thanks.", "start": 832.95, "duration": 1.55}], "Part 2: The Big Picture of Linear Algebra": [{"text": "GILBERT STRANG: OK, in this second part,", "start": 14.83, "duration": 3.54}, {"text": "I'm going to start with linear equations, A times x equal b.", "start": 18.37, "duration": 5.1}, {"text": "And you see actually, the first real good starting point", "start": 23.47, "duration": 3.3}, {"text": "is A times x equals 0.", "start": 26.77, "duration": 2.46}, {"text": "So are there any solutions to the matrix, any combinations", "start": 29.23, "duration": 6.57}, {"text": "of the columns that give 0, any solutions", "start": 35.8, "duration": 3.45}, {"text": "to A times x equals 0?", "start": 39.25, "duration": 2.22}, {"text": "Now, I'm multiplying a matrix A by a vector x in a way", "start": 41.47, "duration": 6.45}, {"text": "you'll know.", "start": 47.92, "duration": 1.25}, {"text": "I take rows of x times, it's called a dot product.", "start": 49.17, "duration": 5.755}, {"text": "Rows of A times x.", "start": 58.33, "duration": 2.67}, {"text": "So I have a row of numbers.", "start": 61.0, "duration": 2.58}, {"text": "And x is a column of numbers.", "start": 63.58, "duration": 2.07}, {"text": "I multiply those numbers and add to get the dot product.", "start": 65.65, "duration": 4.5}, {"text": "And I'm wondering, can I get 0 for each?", "start": 70.15, "duration": 3.48}, {"text": "Is every row-- so having a 0 there", "start": 73.63, "duration": 3.75}, {"text": "is telling me, in geometry, that that row is perpendicular,", "start": 77.38, "duration": 6.11}, {"text": "orthogonal to that column.", "start": 83.49, "duration": 3.64}, {"text": "If a row dot product with a column", "start": 87.13, "duration": 2.6}, {"text": "gives me a 0, then in n dimensional space,", "start": 89.73, "duration": 4.56}, {"text": "that row is perpendicular, 90 degree angle to that column x.", "start": 94.29, "duration": 7.29}, {"text": "So I'm looking to see, are there any vectors x", "start": 101.58, "duration": 3.54}, {"text": "that are perpendicular to all the rows?", "start": 105.12, "duration": 3.0}, {"text": "That's what Ax equals 0 is asking for.", "start": 108.12, "duration": 2.63}, {"text": "Oh, and that's what I've just said right there.", "start": 110.75, "duration": 2.47}, {"text": "I've used the word orthogonal.", "start": 113.22, "duration": 1.62}, {"text": "That's more of a high level word than perpendicular.", "start": 114.84, "duration": 3.07}, {"text": "So I'll stay with that.", "start": 117.91, "duration": 1.69}, {"text": "It sounds a little cooler.", "start": 119.6, "duration": 1.72}, {"text": "OK.", "start": 121.32, "duration": 0.72}, {"text": "And now, we can also look at that transpose.", "start": 122.04, "duration": 5.73}, {"text": "Oh, do you know what the transpose of a matrix is?", "start": 127.77, "duration": 2.94}, {"text": "I take those rows and flip the matrix,", "start": 130.71, "duration": 3.66}, {"text": "so that those rows become the columns.", "start": 134.37, "duration": 3.09}, {"text": "And the columns of A become the rows of A transpose.", "start": 137.46, "duration": 4.17}, {"text": "So I'll look at A transpose times--", "start": 141.63, "duration": 2.72}, {"text": "we'll call it y for the new problem.", "start": 144.35, "duration": 2.83}, {"text": "A transpose y is all 0s.", "start": 147.18, "duration": 2.76}, {"text": "And then the null space will be any vector, any solutions,", "start": 149.94, "duration": 5.55}, {"text": "any y that's perpendicular to the rows of A transpose.", "start": 155.49, "duration": 5.95}, {"text": "So I would need couple of hours of teaching to develop this", "start": 161.44, "duration": 6.7}, {"text": "properly because we're talking here", "start": 168.14, "duration": 2.49}, {"text": "about the fundamental theorem of linear algebra, which tells me", "start": 170.63, "duration": 5.94}, {"text": "that the vectors in the null space,", "start": 176.57, "duration": 2.52}, {"text": "like that, are perpendicular to the vectors.", "start": 179.09, "duration": 4.74}, {"text": "These guys are.", "start": 183.83, "duration": 0.91}, {"text": "That's the row space.", "start": 184.74, "duration": 1.73}, {"text": "Oh, but maybe I have told you.", "start": 186.47, "duration": 2.7}, {"text": "We've said that, from this equation, that tells you", "start": 189.17, "duration": 4.65}, {"text": "the geometry that that row vectors are", "start": 193.82, "duration": 3.51}, {"text": "perpendicular to the x vector, the thing in the null space.", "start": 197.33, "duration": 3.91}, {"text": "So x is there.", "start": 201.24, "duration": 1.34}, {"text": "The rows are there.", "start": 202.58, "duration": 0.99}, {"text": "And they're perpendicular.", "start": 203.57, "duration": 1.8}, {"text": "Now, if I transpose the matrix, remember", "start": 205.37, "duration": 3.48}, {"text": "that means exchanging rows and columns,", "start": 208.85, "duration": 2.16}, {"text": "so I have a new matrix, new size even.", "start": 211.01, "duration": 3.45}, {"text": "It will the same-- but it's a matrix.", "start": 214.46, "duration": 2.16}, {"text": "The same will be true for it.", "start": 216.62, "duration": 1.92}, {"text": "The rows become the columns.", "start": 218.54, "duration": 6.75}, {"text": "And the solutions to the new equation with A transpose", "start": 225.29, "duration": 5.07}, {"text": "go in that space.", "start": 230.36, "duration": 2.58}, {"text": "So then that little perpendicular sign", "start": 232.94, "duration": 2.16}, {"text": "is reminding us of the geometry.", "start": 235.1, "duration": 2.67}, {"text": "So rows perpendicular to the x's.", "start": 237.77, "duration": 3.78}, {"text": "Columns perpendicular to the y's.", "start": 241.55, "duration": 2.16}, {"text": "That's the best.", "start": 243.71, "duration": 0.72}, {"text": "I finally saw the right way to say that.", "start": 244.43, "duration": 2.7}, {"text": "So I have two pairs.", "start": 247.13, "duration": 2.25}, {"text": "And I know how big each of those four things are.", "start": 249.38, "duration": 5.22}, {"text": "Those are the four fundamental subspaces, two null spaces,", "start": 254.6, "duration": 6.859}, {"text": "two solution spaces with 0.", "start": 261.459, "duration": 2.231}, {"text": "Null means 0.", "start": 263.69, "duration": 1.29}, {"text": "So these x's are in the null space because of that 0.", "start": 264.98, "duration": 5.37}, {"text": "Those are the n's.", "start": 270.35, "duration": 2.1}, {"text": "And then this is the column space and the row space.", "start": 272.45, "duration": 4.72}, {"text": "So we've got four spaces altogether, two pairs.", "start": 277.17, "duration": 2.96}, {"text": "And now, you get to see the big picture", "start": 280.13, "duration": 1.8}, {"text": "of linear algebra, where the four fundamental subspaces do", "start": 281.93, "duration": 3.6}, {"text": "their thing.", "start": 285.53, "duration": 0.51}, {"text": "There you go.", "start": 286.04, "duration": 1.87}, {"text": "You can die happy now.", "start": 287.91, "duration": 2.94}, {"text": "The row spaces there, those are rows of the matrix,", "start": 290.85, "duration": 5.34}, {"text": "independent rows of the matrix.", "start": 296.19, "duration": 1.87}, {"text": "That's why I don't put in all the rows.", "start": 298.06, "duration": 2.97}, {"text": "There are m rows.", "start": 301.03, "duration": 1.2}, {"text": "But I only put in independent ones.", "start": 302.23, "duration": 2.34}, {"text": "So that might be a smaller number r, r the rank.", "start": 304.57, "duration": 4.02}, {"text": "And here are the solutions, the guys perpendicular to them.", "start": 308.59, "duration": 3.51}, {"text": "This is the rows of the matrix.", "start": 312.1, "duration": 2.17}, {"text": "These are the vectors perpendicular to it.", "start": 314.27, "duration": 2.63}, {"text": "These are the columns of the matrix.", "start": 316.9, "duration": 2.02}, {"text": "These are the vectors perpendicular to the columns.", "start": 318.92, "duration": 2.81}, {"text": "You see it's just a natural splitting", "start": 321.73, "duration": 2.49}, {"text": "of the whole spaces of vectors into two pieces and two pieces.", "start": 324.22, "duration": 7.36}, {"text": "And I think of the matrix A, when it multiplies stuff there,", "start": 331.58, "duration": 4.1}, {"text": "it gives stuff here.", "start": 335.68, "duration": 1.89}, {"text": "When A multiplies a vector x, you", "start": 337.57, "duration": 3.87}, {"text": "get a combination of the columns.", "start": 341.44, "duration": 2.47}, {"text": "That with the very, very first slide.", "start": 343.91, "duration": 2.4}, {"text": "A times x is a combination of the columns.", "start": 346.31, "duration": 2.84}, {"text": "And then we look at some x's, if there are", "start": 349.15, "duration": 3.24}, {"text": "any, where A times x gives 0.", "start": 352.39, "duration": 4.5}, {"text": "And there's 0 right there.", "start": 356.89, "duration": 2.76}, {"text": "OK.", "start": 359.65, "duration": 1.7}, {"text": "OK, so that's the big picture.", "start": 361.35, "duration": 2.66}, {"text": "And I'll just point to another little point", "start": 364.01, "duration": 5.03}, {"text": "that's hiding in this picture.", "start": 369.04, "duration": 1.66}, {"text": "You see that little symbol there, that little thing,", "start": 370.7, "duration": 2.58}, {"text": "and it's also here?", "start": 373.28, "duration": 2.3}, {"text": "What that means is that those guys are", "start": 375.58, "duration": 2.54}, {"text": "perpendicular to these.", "start": 378.12, "duration": 2.07}, {"text": "And these are perpendicular to these.", "start": 380.19, "duration": 1.65}, {"text": "So we have four subspaces, two pairs, two perpendicular pairs.", "start": 381.84, "duration": 5.04}, {"text": "And that's when you get the idea of knowing what they mean,", "start": 386.88, "duration": 5.19}, {"text": "knowing how to find them, at least for a small matrix,", "start": 392.07, "duration": 4.2}, {"text": "you've got the heart of linear algebra part one.", "start": 396.27, "duration": 5.43}, {"text": "This is the first half of linear algebra.", "start": 401.7, "duration": 2.53}, {"text": "OK, I'll just see what else there is.", "start": 404.23, "duration": 2.81}, {"text": "Oh, here, oh, well, this is another comment.", "start": 407.04, "duration": 4.8}, {"text": "I've hardly told you how to multiply two matrices.", "start": 411.84, "duration": 3.63}, {"text": "The usual way is rows times columns.", "start": 415.47, "duration": 3.51}, {"text": "But linear algebra being always interesting,", "start": 418.98, "duration": 4.05}, {"text": "there's another way that I happen", "start": 423.03, "duration": 2.46}, {"text": "to like, columns times rows.", "start": 425.49, "duration": 4.35}, {"text": "Now, there is a column times a row.", "start": 429.84, "duration": 2.85}, {"text": "Now, column times a row, we've seen that once", "start": 432.69, "duration": 4.17}, {"text": "for that rank one matrix.", "start": 436.86, "duration": 1.96}, {"text": "Do you remember I said that those rank one", "start": 438.82, "duration": 2.42}, {"text": "matrix, one column times is one row are the building blocks?", "start": 441.24, "duration": 3.78}, {"text": "Well, here is the building.", "start": 445.02, "duration": 1.56}, {"text": "Those are n of those blocks.", "start": 446.58, "duration": 3.08}, {"text": "A column times a row, a column times a row.", "start": 449.66, "duration": 2.98}, {"text": "And here is a reminder of the--", "start": 452.64, "duration": 3.09}, {"text": "oh, we've only-- oh, we're coming up", "start": 455.73, "duration": 2.43}, {"text": "to A equal LU, the first one.", "start": 458.16, "duration": 3.36}, {"text": "Get on with it, Professor Strang.", "start": 461.52, "duration": 3.08}, {"text": "OK.", "start": 464.6, "duration": 0.97}, {"text": "OK, now we're solving equations.", "start": 465.57, "duration": 2.13}, {"text": "Now we're going to get L times U. So right.", "start": 467.7, "duration": 5.48}, {"text": "So there's two equations and two unknowns solved in high school", "start": 473.18, "duration": 4.96}, {"text": "and how.", "start": 478.14, "duration": 1.02}, {"text": "Do you remember how?", "start": 479.16, "duration": 1.14}, {"text": "That's the whole point.", "start": 480.3, "duration": 1.5}, {"text": "If I take twice that equation, so it's 4x plus 6y equal 14,", "start": 481.8, "duration": 6.52}, {"text": "and subtract from this one, then I", "start": 488.32, "duration": 2.79}, {"text": "get an easy equation for only y by itself.", "start": 491.11, "duration": 3.87}, {"text": "So that's what I did.", "start": 494.98, "duration": 1.05}, {"text": "That's called elimination.", "start": 496.03, "duration": 1.28}, {"text": "I eliminated this 4x.", "start": 497.31, "duration": 1.98}, {"text": "It's gone.", "start": 499.29, "duration": 1.74}, {"text": "It's 2 times that.", "start": 501.03, "duration": 2.37}, {"text": "That's why I chose to multiply it by 2.", "start": 503.4, "duration": 3.15}, {"text": "Then 2 times this gives me 4 x's.", "start": 506.55, "duration": 3.45}, {"text": "When I subtract it, it's gone and I'm left with 1y equal 1.", "start": 510.0, "duration": 5.25}, {"text": "So I know the answer y equal 1.", "start": 515.25, "duration": 2.73}, {"text": "And then I go backwards to x equal 2 because 2x plus,", "start": 517.98, "duration": 5.1}, {"text": "this is now, 3 equals 7.", "start": 523.08, "duration": 2.82}, {"text": "2x is 4.", "start": 525.9, "duration": 0.93}, {"text": "x is 2.", "start": 526.83, "duration": 2.07}, {"text": "And the real point about linear algebra", "start": 528.9, "duration": 6.6}, {"text": "done right is that all those steps", "start": 535.5, "duration": 5.29}, {"text": "can be expressed as a break up, another way", "start": 540.79, "duration": 3.72}, {"text": "to break up the matrix A into a lower triangular matrix.", "start": 544.51, "duration": 5.35}, {"text": "You see that that matrix is triangular.", "start": 549.86, "duration": 2.24}, {"text": "It's lower triangular.", "start": 552.1, "duration": 1.62}, {"text": "And this one is upper triangular.", "start": 553.72, "duration": 1.71}, {"text": "So those are called L and U. Yeah, yeah.", "start": 555.43, "duration": 3.36}, {"text": "So what we did here is expressed by that matrix multiplication.", "start": 558.79, "duration": 6.03}, {"text": "You really want to express everything, in the end,", "start": 564.82, "duration": 2.73}, {"text": "as multiplying a couple of matrices.", "start": 567.55, "duration": 2.64}, {"text": "Then you know exactly where you are.", "start": 570.19, "duration": 3.18}, {"text": "So that's the idea of elimination.", "start": 573.37, "duration": 3.97}, {"text": "And now, we only were doing a 2 by 2 matrix.", "start": 577.34, "duration": 4.11}, {"text": "You remember our little matrix was pathetic, 2, 3, 4, 7.", "start": 581.45, "duration": 5.21}, {"text": "That was our matrix A. We can't stop there.", "start": 586.66, "duration": 3.45}, {"text": "So linear algebra goes on to matrix of any size.", "start": 590.11, "duration": 5.44}, {"text": "And this is the way to find the triangular", "start": 595.55, "duration": 5.43}, {"text": "factor L and the upper triangular factor", "start": 600.98, "duration": 2.94}, {"text": "U. That would need more time.", "start": 603.92, "duration": 3.81}, {"text": "So all I want to say is, when you're doing elimination", "start": 607.73, "duration": 4.83}, {"text": "solving equations, then in the back of your mind", "start": 612.56, "duration": 3.93}, {"text": "or in the back page, you are producing an L matrix lower", "start": 616.49, "duration": 6.83}, {"text": "and a U matrix upper.", "start": 623.32, "duration": 2.38}, {"text": "So yeah.", "start": 625.7, "duration": 2.85}, {"text": "Let me see.", "start": 628.55, "duration": 0.62}, {"text": "Yeah, here we see them.", "start": 629.17, "duration": 1.3}, {"text": "The L matrix is all 0s above.", "start": 630.47, "duration": 4.68}, {"text": "The U matrix is all 0s below.", "start": 635.15, "duration": 2.52}, {"text": "And that's what is really happening.", "start": 637.67, "duration": 4.41}, {"text": "So that's what computer system totally focuses on.", "start": 642.08, "duration": 4.39}, {"text": "OK, that's the first slide of a new part.", "start": 646.47, "duration": 3.68}, {"text": "So I'll stop here and coming back to orthogonal vectors.", "start": 650.15, "duration": 4.32}, {"text": "Good.", "start": 654.47, "duration": 1.55}], "Part 3: Orthogonal Vectors": [{"text": "GILBERT STRANG: OK, ready for part three of this vision", "start": 14.2, "duration": 6.85}, {"text": "of linear algebra.", "start": 21.05, "duration": 2.38}, {"text": "So the key word in part three is orthogonal, which", "start": 23.43, "duration": 3.38}, {"text": "again means perpendicular.", "start": 26.81, "duration": 2.52}, {"text": "So we have perpendicular vectors.", "start": 29.33, "duration": 2.789}, {"text": "We can imagine those.", "start": 32.119, "duration": 2.011}, {"text": "We have something called orthogonal matrices.", "start": 34.13, "duration": 3.82}, {"text": "That's when-- I've got one here.", "start": 37.95, "duration": 3.23}, {"text": "An orthogonal matrix is when we have these columns.", "start": 41.18, "duration": 4.83}, {"text": "I'm always going to use the letter", "start": 46.01, "duration": 1.74}, {"text": "Q for an orthogonal matrix.", "start": 47.75, "duration": 2.51}, {"text": "And I look at its columns, and every column", "start": 50.26, "duration": 2.65}, {"text": "is perpendicular to every other column.", "start": 52.91, "duration": 3.6}, {"text": "So I don't just have two perpendicular vectors", "start": 56.51, "duration": 3.09}, {"text": "going like this.", "start": 59.6, "duration": 1.17}, {"text": "I have n of them because I'm in n dimensions.", "start": 60.77, "duration": 3.18}, {"text": "And you just imagine xyz axes or xyzw axes,", "start": 63.95, "duration": 5.85}, {"text": "go up to 4D for relativity, go up to 8D", "start": 69.8, "duration": 4.095}, {"text": "for string theory, 8 dimensions.", "start": 73.895, "duration": 3.555}, {"text": "We just have vectors.", "start": 77.45, "duration": 1.38}, {"text": "After all, it's just this row of numbers or a column of numbers.", "start": 78.83, "duration": 4.38}, {"text": "And we can decide when things are perpendicular by that test.", "start": 83.21, "duration": 5.7}, {"text": "Like say the test for Q1 to be perpendicular to Qn", "start": 88.91, "duration": 5.31}, {"text": "is that row times that column.", "start": 94.22, "duration": 4.38}, {"text": "When I say times, I mean dot product, multiply every pair.", "start": 98.6, "duration": 4.76}, {"text": "Q1 transpose Qn gives that 0 up there.", "start": 103.36, "duration": 3.94}, {"text": "So the columns are perpendicular.", "start": 107.3, "duration": 2.98}, {"text": "And those matrices are the best to compute with.", "start": 110.28, "duration": 2.67}, {"text": "And again, they're called Q.", "start": 112.95, "duration": 2.13}, {"text": "And one way to, a quick matrix way,", "start": 115.08, "duration": 3.19}, {"text": "because there's always a matrix way to explain something,", "start": 118.27, "duration": 4.75}, {"text": "and you'll see how quick it is here.", "start": 123.02, "duration": 3.32}, {"text": "This business of having columns that", "start": 126.34, "duration": 2.72}, {"text": "are perpendicular to each other, and actually, I'm", "start": 129.06, "duration": 2.82}, {"text": "going to make those lengths of all those column vectors all 1,", "start": 131.88, "duration": 4.11}, {"text": "just to sort of normalize it.", "start": 135.99, "duration": 2.82}, {"text": "Then all that's expressed by, if I multiply Q transpose by Q,", "start": 138.81, "duration": 6.96}, {"text": "I'm taking all those dot products,", "start": 145.77, "duration": 2.35}, {"text": "and I'm getting 1s when it's Q against itself.", "start": 148.12, "duration": 5.03}, {"text": "And I'm getting 0s when it's one 1 Q versus another Q.", "start": 153.15, "duration": 4.41}, {"text": "And again, just think of three perpendicular axes.", "start": 157.56, "duration": 3.57}, {"text": "Those directions are the Q1, Q2, Q3.", "start": 161.13, "duration": 4.56}, {"text": "OK?", "start": 165.69, "duration": 1.14}, {"text": "So we really want to compute with those.", "start": 166.83, "duration": 3.0}, {"text": "Here's an example.", "start": 169.83, "duration": 1.71}, {"text": "Well, that has just two perpendicular axes.", "start": 171.54, "duration": 2.49}, {"text": "I didn't have space for the third one.", "start": 174.03, "duration": 2.49}, {"text": "So do you see that those two columns are perpendicular?", "start": 176.52, "duration": 6.48}, {"text": "Again, what does that mean?", "start": 183.0, "duration": 1.41}, {"text": "I take the dot product.", "start": 184.41, "duration": 1.62}, {"text": "Minus 1 times 2, 2.", "start": 186.03, "duration": 2.43}, {"text": "2 times minus 1, another minus 2.", "start": 188.46, "duration": 2.22}, {"text": "So I'm up to minus 4 at this point.", "start": 190.68, "duration": 2.19}, {"text": "And then 2 times 2 gives a plus 4.", "start": 192.87, "duration": 1.95}, {"text": "So it all washes out to 0.", "start": 194.82, "duration": 2.4}, {"text": "And why is that 1/3 there?", "start": 197.22, "duration": 2.89}, {"text": "Why is that?", "start": 200.11, "duration": 1.29}, {"text": "That's so that these vectors will have length 1.", "start": 201.4, "duration": 4.39}, {"text": "There will be unit vectors.", "start": 205.79, "duration": 2.18}, {"text": "Yeah, and how do I figure, the length of a vector,", "start": 207.97, "duration": 3.0}, {"text": "just while we're at it?", "start": 210.97, "duration": 2.26}, {"text": "I take 1 squared or minus 1 squared gives me 1.", "start": 213.23, "duration": 3.83}, {"text": "2 squared and 2 squared, I take the dot product with itself.", "start": 217.06, "duration": 5.14}, {"text": "So minus 1 squared, 2 squared, and 2", "start": 222.2, "duration": 1.96}, {"text": "squared, that adds up to 9.", "start": 224.16, "duration": 2.16}, {"text": "The square root of 9 is the length.", "start": 226.32, "duration": 2.25}, {"text": "I'm just doing Pythagoras here.", "start": 228.57, "duration": 2.93}, {"text": "There is one side of a triangle.", "start": 231.5, "duration": 2.38}, {"text": "Here is a second side of a triangle.", "start": 233.88, "duration": 2.26}, {"text": "It's a right triangle because that vector", "start": 236.14, "duration": 2.36}, {"text": "is perpendicular to that one.", "start": 238.5, "duration": 2.01}, {"text": "It's in 3D because they have three components.", "start": 240.51, "duration": 3.55}, {"text": "And I didn't write a third direction.", "start": 244.06, "duration": 3.44}, {"text": "And their length one vectors because just that's", "start": 247.5, "duration": 6.9}, {"text": "how when I compute the length and remember", "start": 254.4, "duration": 2.55}, {"text": "about the 1/3, which is put in there to give a length 1.", "start": 256.95, "duration": 5.069}, {"text": "So OK.", "start": 262.019, "duration": 1.651}, {"text": "So these matrices are with Q transpose times Q equal I.", "start": 263.67, "duration": 4.65}, {"text": "That again, that's the matrix shorthand for all", "start": 268.32, "duration": 2.94}, {"text": "I've just said.", "start": 271.26, "duration": 2.08}, {"text": "And those matrices are the best because they don't", "start": 273.34, "duration": 5.71}, {"text": "change the length of anything.", "start": 279.05, "duration": 1.92}, {"text": "You don't have blow up.", "start": 280.97, "duration": 1.26}, {"text": "You don't have going to 0.", "start": 282.23, "duration": 1.62}, {"text": "You can multiply together 1,000 matrices,", "start": 283.85, "duration": 2.52}, {"text": "and you'll still have another orthogonal matrix.", "start": 286.37, "duration": 4.07}, {"text": "Yes, a little family of beautiful matrices.", "start": 290.44, "duration": 3.37}, {"text": "OK, and very, very useful.", "start": 293.81, "duration": 3.44}, {"text": "OK, and there was a good example.", "start": 297.25, "duration": 2.07}, {"text": "Oh, I think the way I got that example,", "start": 299.32, "duration": 2.67}, {"text": "I just added a third row.", "start": 301.99, "duration": 2.34}, {"text": "The third column, sorry.", "start": 304.33, "duration": 1.23}, {"text": "The third column.", "start": 305.56, "duration": 1.21}, {"text": "So 2 squared plus 2 squared plus minus 1 squared.", "start": 306.77, "duration": 2.91}, {"text": "That adds up to 9.", "start": 309.68, "duration": 1.7}, {"text": "When I take the square root, I get 3.", "start": 311.38, "duration": 2.55}, {"text": "So that has length 3.", "start": 313.93, "duration": 1.72}, {"text": "I divided by 3.", "start": 315.65, "duration": 1.36}, {"text": "So it would have length 1.", "start": 317.01, "duration": 2.02}, {"text": "We always want to see 1s, like we do there.", "start": 319.03, "duration": 4.16}, {"text": "And if I-- here's a simple fact.", "start": 323.19, "duration": 2.92}, {"text": "But great.", "start": 326.11, "duration": 1.96}, {"text": "Then if I have two of these matrices", "start": 328.07, "duration": 2.03}, {"text": "or 50 of these matrices, I could multiply them together.", "start": 330.1, "duration": 3.48}, {"text": "And I would still have length of 1.", "start": 333.58, "duration": 2.34}, {"text": "I'd still have orthogonal matrices.", "start": 335.92, "duration": 2.13}, {"text": "1 times 1 times 1 forever is 1.", "start": 338.05, "duration": 3.54}, {"text": "OK, so there's probably something hiding here.", "start": 341.59, "duration": 3.69}, {"text": "Oh, yeah.", "start": 345.28, "duration": 1.58}, {"text": "Oh, yeah, to understand why these matrices are important,", "start": 346.86, "duration": 4.29}, {"text": "this one, this line is telling me that, if I have a vector x,", "start": 351.15, "duration": 4.83}, {"text": "and I multiply by Q, it doesn't change the length.", "start": 355.98, "duration": 4.88}, {"text": "This is a symbol for length squared.", "start": 360.86, "duration": 2.85}, {"text": "And that's equal to the original length squared.", "start": 363.71, "duration": 2.34}, {"text": "Length it is preserved by these Qs.", "start": 366.05, "duration": 2.46}, {"text": "Everything is preserved.", "start": 368.51, "duration": 1.08}, {"text": "You're multiplying effectively by the matrix versions", "start": 369.59, "duration": 4.32}, {"text": "of 1 and minus 1.", "start": 373.91, "duration": 2.58}, {"text": "And a rotation is a very significant very valuable", "start": 376.49, "duration": 5.94}, {"text": "orthogonal matrix, which just has cosines and signs.", "start": 382.43, "duration": 3.54}, {"text": "And everybody's remembering that cosine squared plus sine", "start": 385.97, "duration": 3.54}, {"text": "squared is 1 from trig.", "start": 389.51, "duration": 3.72}, {"text": "So that's an orthogonal matrix.", "start": 393.23, "duration": 5.12}, {"text": "Oh, it's also orthogonal because the dot", "start": 398.35, "duration": 2.65}, {"text": "product between that one and that one,", "start": 401.0, "duration": 3.03}, {"text": "you're OK for the dot product.", "start": 404.03, "duration": 1.74}, {"text": "That product gives me minus sine cosine, plus sine cosine, 0.", "start": 405.77, "duration": 5.49}, {"text": "So the column 1 is orthogonal to column 2.", "start": 411.26, "duration": 4.35}, {"text": "That's good.", "start": 415.61, "duration": 0.75}, {"text": "OK.", "start": 416.36, "duration": 2.38}, {"text": "These lambdas that you see here are", "start": 418.74, "duration": 4.41}, {"text": "something called eigenvalues.", "start": 423.15, "duration": 1.32}, {"text": "That's not allowed until the next lecture.", "start": 424.47, "duration": 4.0}, {"text": "OK, all right, now, here's something.", "start": 428.47, "duration": 2.82}, {"text": "Here's a computing thing.", "start": 431.29, "duration": 2.22}, {"text": "If we have a bunch of columns, not orthogonal, not length 1,", "start": 433.51, "duration": 7.08}, {"text": "then, often, we would like to convert them", "start": 440.59, "duration": 3.69}, {"text": "to, so we call those, A1 to AN.", "start": 444.28, "duration": 3.39}, {"text": "Nothing special about those columns.", "start": 447.67, "duration": 2.92}, {"text": "We would like to convert them to orthogonal columns", "start": 450.59, "duration": 2.78}, {"text": "because they're the beautiful ones, Q1 up to Qn.", "start": 453.37, "duration": 3.51}, {"text": "And two guys called Graham and Schmidt figured out", "start": 456.88, "duration": 4.23}, {"text": "a way to do that.", "start": 461.11, "duration": 1.02}, {"text": "And a century later, we're still using their idea.", "start": 462.13, "duration": 4.46}, {"text": "Well, I don't know whose idea it was actually.", "start": 466.59, "duration": 1.95}, {"text": "I think Graham had the idea.", "start": 468.54, "duration": 1.38}, {"text": "And I'm not really sure what Schmidt, how he got into it.", "start": 469.92, "duration": 4.35}, {"text": "Well, he may have repeated the idea.", "start": 474.27, "duration": 1.92}, {"text": "So OK, so I won't go all the details.", "start": 476.19, "duration": 5.19}, {"text": "But here's what the point is the point", "start": 481.38, "duration": 3.21}, {"text": "is, if I have a bunch of columns that are independent,", "start": 484.59, "duration": 5.54}, {"text": "they go in different directions, but they're not", "start": 490.13, "duration": 2.0}, {"text": "90 degree directions.", "start": 492.13, "duration": 2.73}, {"text": "Then I can convert it to a 90 degree one", "start": 494.86, "duration": 2.64}, {"text": "to perpendicular axes with a matrix R,", "start": 497.5, "duration": 5.49}, {"text": "happens to be triangular, that did the moving around, did", "start": 502.99, "duration": 6.12}, {"text": "take that combinations.", "start": 509.11, "duration": 1.71}, {"text": "So A equal QR is one of the fundamental steps", "start": 510.82, "duration": 4.44}, {"text": "of linear algebra and computational linear algebra.", "start": 515.26, "duration": 4.71}, {"text": "Very, very often, we're given a matrix", "start": 519.97, "duration": 2.819}, {"text": "A. We want a nice matrix Q, so we do this Graham Schmidt step", "start": 522.789, "duration": 6.001}, {"text": "to make the columns orthogonal.", "start": 528.79, "duration": 2.85}, {"text": "And oh, here's a first step of Graham Schmidt.", "start": 531.64, "duration": 4.44}, {"text": "But you'll need practice to see all the steps.", "start": 536.08, "duration": 7.65}, {"text": "Maybe not.", "start": 543.73, "duration": 1.23}, {"text": "OK, so here, what's the advantage", "start": 544.96, "duration": 4.29}, {"text": "of perpendicular vectors?", "start": 549.25, "duration": 2.37}, {"text": "Suppose I have a triangle.", "start": 551.62, "duration": 2.11}, {"text": "And one side is perpendicular to the second side.", "start": 553.73, "duration": 3.62}, {"text": "How does that help?", "start": 557.35, "duration": 2.52}, {"text": "Well, that's a right triangle then.", "start": 559.87, "duration": 2.55}, {"text": "Side A perpendicular to side B. And of course, Pythagoras, now", "start": 562.42, "duration": 5.31}, {"text": "we're really going back, Pythagoras said,", "start": 567.73, "duration": 2.97}, {"text": "a squared plus b squared is c squared.", "start": 570.7, "duration": 2.62}, {"text": "So we have beautiful formulas when things are perpendicular.", "start": 573.32, "duration": 3.59}, {"text": "If the angles are not 90 degrees when the cosine of 90 degrees", "start": 576.91, "duration": 5.55}, {"text": "is 1 or maybe the sine of 90 degrees is 1,", "start": 582.46, "duration": 4.34}, {"text": "yeah, sine of 90 degrees is 1.", "start": 586.8, "duration": 3.21}, {"text": "For those perfect angles, 0 and 90 degrees,", "start": 590.01, "duration": 6.54}, {"text": "we can do everything.", "start": 596.55, "duration": 2.23}, {"text": "And here is a place that Q fits.", "start": 598.78, "duration": 4.485}, {"text": "This is like the first big application of linear algebra.", "start": 605.98, "duration": 4.68}, {"text": "So let me just say what it is.", "start": 610.66, "duration": 3.24}, {"text": "And it uses these cubes.", "start": 613.9, "duration": 3.39}, {"text": "So what's the application that's called least squares?", "start": 617.29, "duration": 3.27}, {"text": "And you start with equations, Ax equal b.", "start": 620.56, "duration": 5.78}, {"text": "You always think of that as a matrix", "start": 626.34, "duration": 1.76}, {"text": "times the unknown vector, being known,", "start": 628.1, "duration": 2.61}, {"text": "right hand side b Ax equal b.", "start": 630.71, "duration": 3.24}, {"text": "So suppose we have too many equations.", "start": 633.95, "duration": 4.38}, {"text": "That often happens.", "start": 638.33, "duration": 1.02}, {"text": "If you take too many measurements,", "start": 639.35, "duration": 1.59}, {"text": "you want to get an exact x.", "start": 640.94, "duration": 2.8}, {"text": "So you do more and more measurements to b.", "start": 643.74, "duration": 2.78}, {"text": "You're pasting more and more conditions on x.", "start": 646.52, "duration": 2.34}, {"text": "And you're not going to find an exact x because you've", "start": 648.86, "duration": 3.54}, {"text": "got too many equations. m is bigger than n.", "start": 652.4, "duration": 3.29}, {"text": "We might have 2,000 measurements,", "start": 655.69, "duration": 2.29}, {"text": "say, from medical things or from satellites.", "start": 657.98, "duration": 4.18}, {"text": "And we might have only two unknowns,", "start": 662.16, "duration": 2.09}, {"text": "fitting a straight line with only two variables.", "start": 664.25, "duration": 3.18}, {"text": "So how am I going to solve 2,000 equations with two", "start": 667.43, "duration": 3.03}, {"text": "unknowns Well, I'm not.", "start": 670.46, "duration": 2.4}, {"text": "But I look for the best solution.", "start": 672.86, "duration": 3.82}, {"text": "How close can I come?", "start": 676.68, "duration": 1.37}, {"text": "And that's what least squares is about.", "start": 678.05, "duration": 2.32}, {"text": "You get Ax as close as possible to b.", "start": 680.37, "duration": 4.62}, {"text": "And probably, this will show how the--", "start": 684.99, "duration": 4.02}, {"text": "yeah.", "start": 689.01, "duration": 0.83}, {"text": "Yeah, here's the right equation.", "start": 689.84, "duration": 1.96}, {"text": "When you-- here's my message.", "start": 691.8, "duration": 2.92}, {"text": "When you can't solve Ax equal b, multiply both sides", "start": 694.72, "duration": 4.49}, {"text": "by A transpose.", "start": 699.21, "duration": 1.92}, {"text": "Then you can solve this equation.", "start": 701.13, "duration": 2.26}, {"text": "That's the right equation.", "start": 703.39, "duration": 1.37}, {"text": "So I put a little hat on that x to show", "start": 704.76, "duration": 3.27}, {"text": "that it doesn't solve the original equation, Ax equal b,", "start": 708.03, "duration": 4.62}, {"text": "but it comes the closest.", "start": 712.65, "duration": 2.23}, {"text": "It's the closest solution I could find.", "start": 714.88, "duration": 2.42}, {"text": "And it's discovered by multiplying both sides", "start": 717.3, "duration": 4.02}, {"text": "by this A transpose matrix.", "start": 721.32, "duration": 2.16}, {"text": "So A transpose A is a terrifically important matrix.", "start": 723.48, "duration": 3.51}, {"text": "It's a square matrix.", "start": 726.99, "duration": 2.85}, {"text": "See, A didn't have to be square.", "start": 729.84, "duration": 1.57}, {"text": "I could have lots of measurements there,", "start": 731.41, "duration": 2.22}, {"text": "many, many equations, long, thin matrix for A.", "start": 733.63, "duration": 3.57}, {"text": "But A transpose A always comes out square and also symmetric.", "start": 737.2, "duration": 6.31}, {"text": "And it's just a great matrix for theory.", "start": 743.51, "duration": 5.0}, {"text": "And this QR business makes it work in practice.", "start": 748.51, "duration": 4.83}, {"text": "Let me see if there's more.", "start": 753.34, "duration": 1.44}, {"text": "So this is, oh, yeah.", "start": 754.78, "duration": 1.83}, {"text": "This is the geometry.", "start": 756.61, "duration": 3.04}, {"text": "So I start with a matrix A. It's only got a few columns, maybe", "start": 759.65, "duration": 5.78}, {"text": "even only two columns.", "start": 765.43, "duration": 1.74}, {"text": "So its column space is just a plane, not the whole space.", "start": 767.17, "duration": 4.98}, {"text": "But my right hand side b is somewhere else in whole space.", "start": 772.15, "duration": 4.54}, {"text": "You see this situation.", "start": 776.69, "duration": 2.11}, {"text": "I can only solve Ax equal b when b is", "start": 778.8, "duration": 4.19}, {"text": "a combination of the columns.", "start": 782.99, "duration": 1.56}, {"text": "And here, it's not.", "start": 784.55, "duration": 1.56}, {"text": "The measurements weren't perfect.", "start": 786.11, "duration": 1.86}, {"text": "I'm off somewhere.", "start": 787.97, "duration": 1.23}, {"text": "So how do you deal with that?", "start": 789.2, "duration": 3.64}, {"text": "Geometry tells you.", "start": 792.84, "duration": 2.52}, {"text": "You can't deal with b.", "start": 795.36, "duration": 2.58}, {"text": "You can't solve Ax equal b.", "start": 797.94, "duration": 2.19}, {"text": "So you drop a perpendicular.", "start": 800.13, "duration": 2.01}, {"text": "You find the closest point, the projection that", "start": 802.14, "duration": 3.71}, {"text": "in the space where you can solve.", "start": 805.85, "duration": 2.91}, {"text": "So then you solve Ax equal p.", "start": 808.76, "duration": 3.4}, {"text": "That's what least squares is all about, fitting the best", "start": 812.16, "duration": 3.0}, {"text": "straight line, the best parabola,", "start": 815.16, "duration": 2.4}, {"text": "whatever, is all linear algebra of perpendicular", "start": 817.56, "duration": 4.65}, {"text": "things and orthogonal matrices.", "start": 822.21, "duration": 3.25}, {"text": "OK, I think that's what I can say about orthogonal.", "start": 825.46, "duration": 5.15}, {"text": "Well, it'll come in again.", "start": 830.61, "duration": 1.35}, {"text": "Orthogonal matrices, perpendicular columns", "start": 831.96, "duration": 3.12}, {"text": "is so beautiful, but next is coming eigenvectors.", "start": 835.08, "duration": 4.11}, {"text": "And that's another chapter.", "start": 839.19, "duration": 1.62}, {"text": "So I'll stop here.", "start": 840.81, "duration": 1.6}, {"text": "Good.", "start": 842.41, "duration": 0.5}, {"text": "Thanks.", "start": 842.91, "duration": 1.55}], "Part 4: Eigenvalues and Eigenvectors": [{"text": "GILBERT STRANG: Moving now to the second half", "start": 13.665, "duration": 1.875}, {"text": "of linear algebra.", "start": 15.54, "duration": 1.68}, {"text": "It's about eigenvalues and eigenvectors.", "start": 17.22, "duration": 2.94}, {"text": "The first half, I just had a matrix.", "start": 20.16, "duration": 2.16}, {"text": "I solved equations.", "start": 22.32, "duration": 1.86}, {"text": "The second half, you'll see the point", "start": 24.18, "duration": 2.97}, {"text": "of eigenvalues and eigenvectors as a new way", "start": 27.15, "duration": 2.67}, {"text": "to look deeper into the matrix to see what's important there.", "start": 29.82, "duration": 4.68}, {"text": "OK, so what are they?", "start": 34.5, "duration": 2.16}, {"text": "This is a big equation, S time x.", "start": 36.66, "duration": 3.85}, {"text": "So S is our matrix.", "start": 40.51, "duration": 2.08}, {"text": "And I've called it S because I'm taking", "start": 42.59, "duration": 2.47}, {"text": "it to be a symmetric matrix.", "start": 45.06, "duration": 2.34}, {"text": "What's on one side of the diagonal", "start": 47.4, "duration": 2.52}, {"text": "is also on the other side of the diagonal.", "start": 49.92, "duration": 2.61}, {"text": "So those have the beautiful properties.", "start": 52.53, "duration": 2.27}, {"text": "Those are the kings of linear algebra.", "start": 54.8, "duration": 2.47}, {"text": "Now, about eigenvectors x and eigenvalues lambda.", "start": 57.27, "duration": 5.01}, {"text": "So what does that equation, Sx equal lambda x, tell me?", "start": 62.28, "duration": 4.53}, {"text": "That says that I have a special vector x.", "start": 66.81, "duration": 3.74}, {"text": "When I multiply it by S, my matrix,", "start": 70.55, "duration": 3.85}, {"text": "I stay in the same direction as the original x.", "start": 74.4, "duration": 3.33}, {"text": "It might get multiplied by 2.", "start": 77.73, "duration": 2.08}, {"text": "Lambda could be 2.", "start": 79.81, "duration": 1.82}, {"text": "It might get multiplied by 0.", "start": 81.63, "duration": 2.04}, {"text": "Lambda there could even be 0.", "start": 83.67, "duration": 1.92}, {"text": "It might get multiplied by minus 2, whatever.", "start": 85.59, "duration": 3.15}, {"text": "But it's along the same line.", "start": 88.74, "duration": 1.94}, {"text": "So that's like taking a matrix and discovering inside it", "start": 90.68, "duration": 5.08}, {"text": "something that stays on a line.", "start": 95.76, "duration": 3.98}, {"text": "That means that it's really a sort of one dimensional problem", "start": 99.74, "duration": 3.9}, {"text": "if we're looking along that eigenvector.", "start": 103.64, "duration": 3.42}, {"text": "And that makes computations infinitely easier.", "start": 107.06, "duration": 4.32}, {"text": "The hard part of a matrix is all the connections", "start": 111.38, "duration": 3.69}, {"text": "between different rows and columns.", "start": 115.07, "duration": 2.76}, {"text": "So eigenvectors are the guys that", "start": 117.83, "duration": 2.22}, {"text": "stay in that same direction.", "start": 120.05, "duration": 2.172}, {"text": "And y is another eigenvector.", "start": 124.83, "duration": 3.5}, {"text": "It has its own eigenvalue.", "start": 128.33, "duration": 1.839}, {"text": "It got multiplied by alpha where Sx multiplied the x", "start": 130.169, "duration": 4.031}, {"text": "by some other number lambda.", "start": 134.2, "duration": 2.34}, {"text": "So there's our couple of eigenvectors.", "start": 136.54, "duration": 1.86}, {"text": "And the beautiful fact is that because S is symmetric,", "start": 138.4, "duration": 5.2}, {"text": "those two eigenvectors are perpendicular.", "start": 143.6, "duration": 2.7}, {"text": "They are orthogonal, as it says up there.", "start": 146.3, "duration": 3.15}, {"text": "So symmetric matrices are really the best", "start": 149.45, "duration": 3.0}, {"text": "because their eigenvectors are perpendicular.", "start": 152.45, "duration": 2.88}, {"text": "And we have a bunch of one dimensional problems.", "start": 155.33, "duration": 3.27}, {"text": "And here, I've included a proof.", "start": 158.6, "duration": 3.18}, {"text": "You want a proof that the eigenvectors are perpendicular?", "start": 161.78, "duration": 5.68}, {"text": "So what does perpendicular mean?", "start": 167.46, "duration": 1.52}, {"text": "It means that x transpose times y, the dot product is 0.", "start": 168.98, "duration": 7.0}, {"text": "The angle is 90 degrees.", "start": 175.98, "duration": 2.96}, {"text": "The cosine is 1.", "start": 178.94, "duration": 2.7}, {"text": "OK.", "start": 181.64, "duration": 1.41}, {"text": "How to show the cosine might be there.", "start": 183.05, "duration": 4.62}, {"text": "How to show that?", "start": 187.67, "duration": 1.89}, {"text": "Yeah, proof.", "start": 189.56, "duration": 0.94}, {"text": "This is just you can tune out for two minutes", "start": 190.5, "duration": 3.23}, {"text": "if you hate proofs.", "start": 193.73, "duration": 1.68}, {"text": "OK, I start with what I know.", "start": 195.41, "duration": 2.58}, {"text": "What I know is in that box.", "start": 197.99, "duration": 2.21}, {"text": "Sx is lambda x.", "start": 200.2, "duration": 1.44}, {"text": "That's one eigenvector.", "start": 201.64, "duration": 1.62}, {"text": "That tells me the eigenvector y.", "start": 203.26, "duration": 2.31}, {"text": "This tells me the eigenvalues are different.", "start": 205.57, "duration": 2.37}, {"text": "And that tells me the matrix is symmetric.", "start": 207.94, "duration": 2.49}, {"text": "I'm just going to juggle those four facts.", "start": 210.43, "duration": 3.21}, {"text": "And I'll end up with x transpose y equals 0.", "start": 213.64, "duration": 5.4}, {"text": "That's orthogonality.", "start": 219.04, "duration": 2.31}, {"text": "OK.", "start": 221.35, "duration": 1.05}, {"text": "So I'll just do it quickly, too quickly.", "start": 222.4, "duration": 3.6}, {"text": "So I take this first thing, and I transpose", "start": 226.0, "duration": 3.9}, {"text": "it, turn it into row vectors.", "start": 229.9, "duration": 2.84}, {"text": "And then when I transpose it, that transpose", "start": 232.74, "duration": 4.71}, {"text": "means I flip rows and columns.", "start": 237.45, "duration": 1.77}, {"text": "But for as symmetric matrix, no different.", "start": 239.22, "duration": 3.45}, {"text": "So S transpose is the same as S.", "start": 242.67, "duration": 2.72}, {"text": "And then I look at this one, and I multiply that", "start": 245.39, "duration": 3.45}, {"text": "by x transpose, both sides by x transpose.", "start": 248.84, "duration": 4.29}, {"text": "And what I end up with is recognizing", "start": 253.13, "duration": 3.57}, {"text": "that lambda times that dot product", "start": 256.7, "duration": 2.55}, {"text": "equals alpha times that dot product.", "start": 259.25, "duration": 2.89}, {"text": "But lambda is different from alpha.", "start": 262.14, "duration": 2.55}, {"text": "So the only way lambda times that number", "start": 264.69, "duration": 2.07}, {"text": "could equal alpha times that number", "start": 266.76, "duration": 1.71}, {"text": "is that number has to be 0.", "start": 268.47, "duration": 2.58}, {"text": "And that's the answer.", "start": 271.05, "duration": 1.41}, {"text": "OK, so that's the proof that used", "start": 272.46, "duration": 2.28}, {"text": "exactly every fact we knew.", "start": 274.74, "duration": 3.24}, {"text": "End of proof.", "start": 277.98, "duration": 1.95}, {"text": "Main point to remember, eigenvectors", "start": 279.93, "duration": 2.79}, {"text": "are perpendicular when the matrix is symmetric.", "start": 282.72, "duration": 4.41}, {"text": "OK.", "start": 287.13, "duration": 2.58}, {"text": "In that case, now, you always want to express these facts", "start": 289.71, "duration": 4.47}, {"text": "as from multiplying matrices.", "start": 294.18, "duration": 4.85}, {"text": "That says everything in a few symbols", "start": 299.03, "duration": 3.15}, {"text": "where I had to use all those words on the previous slide.", "start": 302.18, "duration": 3.82}, {"text": "So that's the result that I'm shooting for,", "start": 306.0, "duration": 4.95}, {"text": "that a symmetric matrix--", "start": 310.95, "duration": 3.5}, {"text": "just focus on that box.", "start": 314.45, "duration": 4.7}, {"text": "A symmetric matrix can be broken up into its eigenvectors.", "start": 319.15, "duration": 5.59}, {"text": "Those are in Q. Its eigenvalues.", "start": 324.74, "duration": 3.0}, {"text": "Those are the lambdas.", "start": 327.74, "duration": 1.17}, {"text": "Those are the numbers lambda 1 to lambda n", "start": 328.91, "duration": 3.21}, {"text": "on the diagonal of lambda.", "start": 332.12, "duration": 1.95}, {"text": "And then the transpose, so the eigenvectors are now", "start": 334.07, "duration": 2.82}, {"text": "rows in Q transpose.", "start": 336.89, "duration": 2.55}, {"text": "That's just perfect.", "start": 339.44, "duration": 2.72}, {"text": "Perfect.", "start": 342.16, "duration": 1.32}, {"text": "Every symmetric matrix is an orthogonal matrix", "start": 343.48, "duration": 3.54}, {"text": "times a diagonal matrix times the transpose", "start": 347.02, "duration": 4.47}, {"text": "of the orthogonal matrix.", "start": 351.49, "duration": 2.19}, {"text": "Yeah, that's called the spectral theorem.", "start": 353.68, "duration": 2.13}, {"text": "And you could say it's up there with the most important facts", "start": 355.81, "duration": 4.86}, {"text": "in linear algebra and in wider mathematics.", "start": 360.67, "duration": 3.74}, {"text": "Yeah, so that's the fact that controls what we do here.", "start": 364.41, "duration": 8.17}, {"text": "Oh, now I have to say what's the situation if the matrix is not", "start": 372.58, "duration": 5.58}, {"text": "symmetric.", "start": 378.16, "duration": 1.7}, {"text": "Now I am not going to get perpendicular eigenvectors.", "start": 379.86, "duration": 4.31}, {"text": "That was a symmetric thing mostly.", "start": 384.17, "duration": 3.3}, {"text": "But I'll get eigenvectors.", "start": 387.47, "duration": 2.79}, {"text": "So I'll get Ax equal lambda x.", "start": 390.26, "duration": 4.88}, {"text": "The first one won't be perpendicular", "start": 395.14, "duration": 1.92}, {"text": "to the second one.", "start": 397.06, "duration": 0.81}, {"text": "The matrix A, it has to be square,", "start": 397.87, "duration": 2.65}, {"text": "or this doesn't make sense.", "start": 400.52, "duration": 1.19}, {"text": "So eigenvalues and eigenvectors are the way", "start": 401.71, "duration": 3.36}, {"text": "to break up a square matrix and find this diagonal matrix", "start": 405.07, "duration": 5.52}, {"text": "lambda with the eigenvalues, lambda 1, lambda 2, to lambda", "start": 410.59, "duration": 4.05}, {"text": "n.", "start": 414.64, "duration": 0.63}, {"text": "That's the purpose.", "start": 415.27, "duration": 3.63}, {"text": "And eigenvectors are perpendicular when", "start": 418.9, "duration": 2.61}, {"text": "it's a symmetric matrix.", "start": 421.51, "duration": 1.74}, {"text": "Otherwise, I just have x and its inverse matrix but no symmetry.", "start": 423.25, "duration": 5.46}, {"text": "OK.", "start": 428.71, "duration": 0.78}, {"text": "So that's the quick expression, another factorization", "start": 429.49, "duration": 4.98}, {"text": "of eigenvalues in lambda.", "start": 434.47, "duration": 3.06}, {"text": "Diagonal, just numbers.", "start": 437.53, "duration": 2.14}, {"text": "And eigenvectors in the columns of x.", "start": 439.67, "duration": 3.7}, {"text": "And now I'm not going to-- oh, I was", "start": 443.37, "duration": 4.24}, {"text": "going to say I'm not going to solve all", "start": 447.61, "duration": 1.92}, {"text": "the problems of applied math.", "start": 449.53, "duration": 1.5}, {"text": "But that's what these are for.", "start": 451.03, "duration": 2.82}, {"text": "Let's just see what's special here about these eigenvectors.", "start": 453.85, "duration": 4.55}, {"text": "Suppose I multiply again by A. I Start with Ax equal lambda x.", "start": 458.4, "duration": 8.4}, {"text": "Now I'm going to multiply both sides by A.", "start": 466.8, "duration": 2.85}, {"text": "That'll tell me something about eigenvalues of A squared.", "start": 469.65, "duration": 3.9}, {"text": "Because when I multiply by A--", "start": 473.55, "duration": 2.42}, {"text": "so let me start with A squared now", "start": 475.97, "duration": 2.26}, {"text": "times x, which means A times Ax.", "start": 478.23, "duration": 4.58}, {"text": "A times Ax.", "start": 482.81, "duration": 1.59}, {"text": "But Ax is lambda x.", "start": 484.4, "duration": 2.04}, {"text": "So I have A times lambda x.", "start": 486.44, "duration": 2.88}, {"text": "And I pull out that number lambda.", "start": 489.32, "duration": 2.86}, {"text": "And I still have a 1Ax.", "start": 492.18, "duration": 2.86}, {"text": "And that's also still lambda x.", "start": 495.04, "duration": 2.41}, {"text": "You see I'm just talking around in a little circle", "start": 497.45, "duration": 2.73}, {"text": "here, just using Ax equal lambda x a couple of times.", "start": 500.18, "duration": 3.9}, {"text": "And the result is--", "start": 504.08, "duration": 1.78}, {"text": "do you see what that means, that result?", "start": 505.86, "duration": 2.54}, {"text": "That means that the eigenvalue for A squared, same eigenvector", "start": 508.4, "duration": 4.68}, {"text": "x.", "start": 513.08, "duration": 0.75}, {"text": "The eigenvalue is lambda squared.", "start": 513.83, "duration": 3.4}, {"text": "And if I add A cubed, the eigenvalue", "start": 517.23, "duration": 2.27}, {"text": "would come out lambda cubed.", "start": 519.5, "duration": 1.799}, {"text": "And if I have a to the-- yeah, yeah.", "start": 521.299, "duration": 3.691}, {"text": "So if I had A to the n times, n multiplies-- so when", "start": 524.99, "duration": 4.65}, {"text": "would you have A to a high power?", "start": 529.64, "duration": 3.45}, {"text": "That's a interesting matrix.", "start": 533.09, "duration": 3.39}, {"text": "Take a matrix and square it, cube it,", "start": 536.48, "duration": 2.43}, {"text": "take high powers of it.", "start": 538.91, "duration": 3.54}, {"text": "The eigenvectors don't change.", "start": 542.45, "duration": 2.07}, {"text": "That's the great thing.", "start": 544.52, "duration": 1.17}, {"text": "That's the whole point of eigenvectors.", "start": 545.69, "duration": 2.16}, {"text": "They don't change.", "start": 547.85, "duration": 1.27}, {"text": "And the eigenvalues just get taken to the high power.", "start": 549.12, "duration": 3.42}, {"text": "So for example, we could ask the question, when,", "start": 552.54, "duration": 3.47}, {"text": "if I multiply a matrix by itself over and over and over again,", "start": 556.01, "duration": 3.15}, {"text": "when do I approach 0?", "start": 559.16, "duration": 1.68}, {"text": "Well, if these numbers are below 1.", "start": 560.84, "duration": 4.08}, {"text": "So eigenvectors, eigenvalues gives you something", "start": 564.92, "duration": 2.34}, {"text": "that you just could not see by those column operations or L", "start": 567.26, "duration": 6.07}, {"text": "times U. This is looking deeper.", "start": 573.33, "duration": 3.65}, {"text": "OK.", "start": 576.98, "duration": 1.14}, {"text": "And OK, and then you'll see we have almost already seen", "start": 578.12, "duration": 6.92}, {"text": "with least squares, this combination A transpose A. So", "start": 585.04, "duration": 4.56}, {"text": "remember A is a rectangular matrix, m by n.", "start": 589.6, "duration": 4.15}, {"text": "I multiply it by its transpose.", "start": 593.75, "duration": 2.47}, {"text": "When I transpose it, I have n by m.", "start": 596.22, "duration": 6.53}, {"text": "And when I multiply them together, I get n by n.", "start": 602.75, "duration": 2.7}, {"text": "So A transpose A is, for theory, is a great matrix,", "start": 605.45, "duration": 5.43}, {"text": "A transpose times A. It's symmetric.", "start": 610.88, "duration": 3.15}, {"text": "Yeah, let's just see what we have about A.", "start": 614.03, "duration": 2.4}, {"text": "It's square for sure.", "start": 616.43, "duration": 3.03}, {"text": "Oh, yeah.", "start": 619.46, "duration": 0.54}, {"text": "This tells me that it's symmetric.", "start": 620.0, "duration": 2.125}, {"text": "And you remember why.", "start": 622.125, "duration": 0.875}, {"text": "I'm always looking for symmetric matrices", "start": 623.0, "duration": 3.02}, {"text": "because they have those orthogonal eigenvectors.", "start": 626.02, "duration": 3.51}, {"text": "They're the beautiful ones for eigenvectors.", "start": 629.53, "duration": 3.4}, {"text": "And A transpose A, automatically symmetric.", "start": 632.93, "duration": 3.14}, {"text": "You just you're multiplying something", "start": 636.07, "duration": 4.59}, {"text": "by its adjoint, its transpose, and the result", "start": 640.66, "duration": 5.28}, {"text": "is that this matrix is symmetric.", "start": 645.94, "duration": 5.07}, {"text": "And maybe there's even more about A transpose A. Yes.", "start": 651.01, "duration": 4.91}, {"text": "What is that?", "start": 655.92, "duration": 0.66}, {"text": "Here is a final--", "start": 660.61, "duration": 2.65}, {"text": "I always say certain matrices are important,", "start": 663.26, "duration": 3.76}, {"text": "but these are the winners.", "start": 667.02, "duration": 3.31}, {"text": "They are symmetric matrices.", "start": 670.33, "duration": 2.18}, {"text": "If I want beautiful matrices, make them symmetric", "start": 672.51, "duration": 3.75}, {"text": "and make the eigenvalues positive.", "start": 676.26, "duration": 2.69}, {"text": "Or non-negative allows 0.", "start": 681.54, "duration": 4.62}, {"text": "So I can either say positive definite", "start": 686.16, "duration": 2.73}, {"text": "when the eigenvalues are positive,", "start": 688.89, "duration": 2.43}, {"text": "or I can say non-negative, which allows 0.", "start": 691.32, "duration": 3.56}, {"text": "And so I have greater than or equal to 0.", "start": 694.88, "duration": 3.82}, {"text": "I just want to say that bringing all", "start": 698.7, "duration": 3.12}, {"text": "the pieces of linear algebra come together", "start": 701.82, "duration": 2.7}, {"text": "in these matrices.", "start": 704.52, "duration": 1.8}, {"text": "And we're seeing the eigenvalue part of it.", "start": 706.32, "duration": 3.45}, {"text": "And here, I've mentioned something called the energy.", "start": 709.77, "duration": 3.25}, {"text": "So that's a physical quantity that", "start": 713.02, "duration": 2.36}, {"text": "also is greater or equal to 0.", "start": 715.38, "duration": 2.85}, {"text": "So that's A transpose A is the matrix", "start": 718.23, "duration": 4.74}, {"text": "that I'm going to use in the final part of this video", "start": 722.97, "duration": 7.32}, {"text": "to achieve the greatest factorization.", "start": 730.29, "duration": 4.77}, {"text": "Q lambda, Q transpose was fantastic.", "start": 735.06, "duration": 3.72}, {"text": "But for a non-square matrix, it's not.", "start": 738.78, "duration": 4.05}, {"text": "For a non-square matrix, they don't even", "start": 742.83, "duration": 2.43}, {"text": "have eigenvalues and eigenvectors.", "start": 745.26, "duration": 2.58}, {"text": "But data comes in non-square matrices.", "start": 747.84, "duration": 3.81}, {"text": "Data is about like we have a bunch of diseases", "start": 751.65, "duration": 2.58}, {"text": "and a bunch of patients or a bunch of medicines.", "start": 754.23, "duration": 3.71}, {"text": "And the number of medicines is not", "start": 757.94, "duration": 1.53}, {"text": "equal the number of patients or diseases.", "start": 759.47, "duration": 2.62}, {"text": "Those are different numbers.", "start": 762.09, "duration": 1.65}, {"text": "So the matrices that we see in data are rectangular.", "start": 763.74, "duration": 4.82}, {"text": "And eigenvalues don't make sense for those.", "start": 768.56, "duration": 4.24}, {"text": "And singular values take the place of eigenvalues.", "start": 772.8, "duration": 3.85}, {"text": "So singular values, and my hope is", "start": 776.65, "duration": 2.63}, {"text": "that linear algebra courses, 18.06 for sure,", "start": 779.28, "duration": 5.27}, {"text": "will always reach, after you explain", "start": 784.55, "duration": 4.14}, {"text": "eigenvalues that everybody agrees is important,", "start": 788.69, "duration": 3.48}, {"text": "get singular values into the course", "start": 792.17, "duration": 2.43}, {"text": "because they really have come on as the big things", "start": 794.6, "duration": 4.14}, {"text": "to do in data.", "start": 798.74, "duration": 1.86}, {"text": "So that would be the last part of this summary video", "start": 800.6, "duration": 6.77}, {"text": "for 2020 vision of linear algebra", "start": 807.37, "duration": 3.41}, {"text": "is to get singular values in there.", "start": 810.78, "duration": 2.74}, {"text": "OK, that's coming next.", "start": 813.52, "duration": 2.6}], "Part 5: Singular Values and Singular Vectors": [{"text": "GILBERT STRANG: OK, so I was speaking", "start": 13.972, "duration": 2.468}, {"text": "about eigenvalues and eigenvectors", "start": 16.44, "duration": 3.21}, {"text": "for a square matrix.", "start": 19.65, "duration": 2.04}, {"text": "And then I said for data for many other applications,", "start": 21.69, "duration": 4.07}, {"text": "the matrices are not square.", "start": 25.76, "duration": 1.63}, {"text": "We need something that replaces eigenvalues and eigenvectors.", "start": 27.39, "duration": 4.17}, {"text": "And what they are--", "start": 31.56, "duration": 1.08}, {"text": "and it's perfect-- is singular values and singular vectors.", "start": 32.64, "duration": 4.89}, {"text": "So may I explain singular values and singular vectors?", "start": 37.53, "duration": 4.09}, {"text": "This slide shows a lot of them.", "start": 41.62, "duration": 4.31}, {"text": "The point is that there will be--", "start": 45.93, "duration": 5.08}, {"text": "now I don't say eigenvectors-- two-- different left singular", "start": 51.01, "duration": 3.42}, {"text": "vectors.", "start": 54.43, "duration": 0.95}, {"text": "They will go into this matrix u.", "start": 55.38, "duration": 2.89}, {"text": "Right singular vectors will go into v.", "start": 58.27, "duration": 3.47}, {"text": "It was the other case that was so special.", "start": 61.74, "duration": 2.14}, {"text": "When the matrix was symmetric, then the left", "start": 63.88, "duration": 2.94}, {"text": "equals left eigenvector.", "start": 66.82, "duration": 1.43}, {"text": "They're the same as the right one.", "start": 68.25, "duration": 2.47}, {"text": "That's sort of sensible.", "start": 70.72, "duration": 1.71}, {"text": "But a general matrix and certainly", "start": 72.43, "duration": 2.31}, {"text": "a rectangular matrix--", "start": 74.74, "duration": 1.395}, {"text": "well, we don't call them eigenvectors,", "start": 79.27, "duration": 1.83}, {"text": "because that would be confusing-- we", "start": 81.1, "duration": 1.5}, {"text": "call them singular vectors.", "start": 82.6, "duration": 1.92}, {"text": "And then, inbetween are not eigenvalues,", "start": 84.52, "duration": 4.23}, {"text": "but singular values.", "start": 88.75, "duration": 2.7}, {"text": "Oh, right.", "start": 91.45, "duration": 0.68}, {"text": "Oh, hiding over here is a key.", "start": 92.13, "duration": 2.57}, {"text": "A times the v's gives sigma times the u's.", "start": 94.7, "duration": 5.42}, {"text": "So that's the replacement for ax equal lambda", "start": 100.12, "duration": 3.09}, {"text": "x, which had x on both sides.", "start": 103.21, "duration": 2.3}, {"text": "OK, now we've got two.", "start": 105.51, "duration": 2.82}, {"text": "But the beauty is now we've got two of those to work with.", "start": 108.33, "duration": 3.49}, {"text": "We can make all the u's orthogonal to each other--", "start": 111.82, "duration": 5.46}, {"text": "all the v's orthogonal to each other.", "start": 117.28, "duration": 3.72}, {"text": "We can do what only symmetric matrices", "start": 121.0, "duration": 2.91}, {"text": "could do for eigenvectors.", "start": 123.91, "duration": 2.25}, {"text": "We can do it now for all matrices,", "start": 126.16, "duration": 2.129}, {"text": "not even squares, just this is where life is, OK.", "start": 128.289, "duration": 4.861}, {"text": "And these numbers instead of the lambdas", "start": 133.15, "duration": 2.19}, {"text": "are called singular values.", "start": 135.34, "duration": 2.01}, {"text": "And we use the letter sigma for those.", "start": 137.35, "duration": 2.94}, {"text": "And here is a picture of the geometry in 2 by 2", "start": 140.29, "duration": 3.82}, {"text": "if we had a 2 by 2 matrix.", "start": 144.11, "duration": 2.49}, {"text": "So you remember, factorization breaks", "start": 146.6, "duration": 3.26}, {"text": "up a matrix into separate small parts,", "start": 149.86, "duration": 4.1}, {"text": "each doing its own thing.", "start": 153.96, "duration": 2.2}, {"text": "So if I multiply by vector x, the first thing", "start": 156.16, "duration": 3.47}, {"text": "that's going to hit it is v transpose.", "start": 159.63, "duration": 2.64}, {"text": "V transpose is an orthogonal matrix.", "start": 162.27, "duration": 3.39}, {"text": "Remember, I said we can make these singular vectors", "start": 165.66, "duration": 3.27}, {"text": "perpendicular.", "start": 168.93, "duration": 1.23}, {"text": "That's what an orthogonal matrix-- so it's just", "start": 170.16, "duration": 2.04}, {"text": "like a rotation that you see.", "start": 172.2, "duration": 2.19}, {"text": "So the v transpose is just turns the vector to get here", "start": 174.39, "duration": 5.9}, {"text": "to get to the second one.", "start": 180.29, "duration": 1.6}, {"text": "Then I'm multiplying by the lambdas.", "start": 181.89, "duration": 2.31}, {"text": "But they're not lambdas now.", "start": 184.2, "duration": 1.29}, {"text": "They're sigma.", "start": 185.49, "duration": 1.55}, {"text": "The matrix, so that's capital sigma.", "start": 187.04, "duration": 3.32}, {"text": "So there is sigma 1 and sigma 2.", "start": 190.36, "duration": 2.58}, {"text": "What they do is stretch the circle.", "start": 192.94, "duration": 3.2}, {"text": "It's a diagonal matrix.", "start": 196.14, "duration": 1.33}, {"text": "So it doesn't turn things.", "start": 197.47, "duration": 2.55}, {"text": "But it stretches the circle to an ellipse", "start": 200.02, "duration": 3.57}, {"text": "because it gets the two different singular values", "start": 203.59, "duration": 3.12}, {"text": "in-- sigma 1 and sigma 2.", "start": 206.71, "duration": 1.77}, {"text": "And then the last guy, the u is going to hit last.", "start": 208.48, "duration": 6.0}, {"text": "It takes the ellipse and turns out again.", "start": 214.48, "duration": 2.34}, {"text": "It's again a rotation--", "start": 216.82, "duration": 1.53}, {"text": "rotation, stretch, rotation.", "start": 218.35, "duration": 3.408}, {"text": "I'll say it again--", "start": 221.758, "duration": 0.792}, {"text": "rotation, stretch, rotation.", "start": 222.55, "duration": 2.59}, {"text": "That's what singular values and singular", "start": 225.14, "duration": 2.72}, {"text": "vectors do, the singular value decomposition.", "start": 227.86, "duration": 3.57}, {"text": "And it's got the best of all worlds here.", "start": 231.43, "duration": 5.31}, {"text": "It's got the rotations, the orthogonal matrices.", "start": 236.74, "duration": 5.01}, {"text": "And it's got the stretches, the diagonal matrices.", "start": 241.75, "duration": 4.253}, {"text": "Compared to those two, those are the greatest.", "start": 246.003, "duration": 1.917}, {"text": "Triangular matrices were good when we were young an hour ago.", "start": 247.92, "duration": 5.37}, {"text": "Now, we're seeing the best.", "start": 253.29, "duration": 2.38}, {"text": "OK, now let me just show you where they come from.", "start": 255.67, "duration": 4.344}, {"text": "Oh, so how to find these v's.", "start": 260.014, "duration": 3.766}, {"text": "Well, the answer is, if I'm looking for orthogonal vectors,", "start": 263.78, "duration": 4.19}, {"text": "the great idea is find a symmetric matrix", "start": 267.97, "duration": 4.57}, {"text": "and with those eigenvectors.", "start": 272.54, "duration": 2.46}, {"text": "So these v's that I want for A are actually eigenvectors", "start": 275.0, "duration": 5.28}, {"text": "of this symmetric matrix A transpose times", "start": 280.28, "duration": 2.82}, {"text": "A. That's just nice.", "start": 283.1, "duration": 2.46}, {"text": "So we can find those singular vectors", "start": 285.56, "duration": 2.79}, {"text": "just as fast as we can find eigenvectors", "start": 288.35, "duration": 2.76}, {"text": "for a symmetric matrix.", "start": 291.11, "duration": 1.59}, {"text": "And we know there, because A transpose A is symmetric.", "start": 292.7, "duration": 3.81}, {"text": "We know the eigenvectors are perpendicular", "start": 296.51, "duration": 3.03}, {"text": "to each other orthonormal.", "start": 299.54, "duration": 2.04}, {"text": "OK, and now what about the other ones because remember,", "start": 301.58, "duration": 3.33}, {"text": "we have two sets.", "start": 304.91, "duration": 1.25}, {"text": "The u's-- well, we just multiply by A. And we've got the u's.", "start": 306.16, "duration": 6.94}, {"text": "Well, and divide by sigmas, because these vectors u's", "start": 313.1, "duration": 3.18}, {"text": "and v's are unit vectors, length one.", "start": 316.28, "duration": 3.42}, {"text": "So we have to scale them properly.", "start": 319.7, "duration": 2.4}, {"text": "And this was a little key bit of algebra to check that,", "start": 322.1, "duration": 5.28}, {"text": "not only the v's were orthogonal,", "start": 327.38, "duration": 2.28}, {"text": "but the u's are orthogonal.", "start": 329.66, "duration": 2.22}, {"text": "Yeah, it just comes out--", "start": 331.88, "duration": 1.44}, {"text": "comes out.", "start": 333.32, "duration": 1.27}, {"text": "So this singular value decomposition,", "start": 334.59, "duration": 2.28}, {"text": "which is maybe, well, say 100 years old, maybe a bit more.", "start": 336.87, "duration": 4.88}, {"text": "But it's really in the last 20, 30 years that singular values", "start": 341.75, "duration": 6.66}, {"text": "have become so important.", "start": 348.41, "duration": 1.75}, {"text": "This is the best factorization of them all.", "start": 350.16, "duration": 4.052}, {"text": "And that's not always reflected in linear algebra courses.", "start": 354.212, "duration": 3.408}, {"text": "So part of my goal today is to say get to singular values.", "start": 357.62, "duration": 6.92}, {"text": "If you've done symmetric matrices and their eigenvalues,", "start": 364.54, "duration": 4.24}, {"text": "then you can do singular values.", "start": 368.78, "duration": 2.44}, {"text": "And I think that's absolutely worth doing, OK, yeah,", "start": 371.22, "duration": 6.63}, {"text": "so and remembering down here that capital Sigma stands", "start": 377.85, "duration": 5.0}, {"text": "for the diagonal matrix of these positive numbers, sigma 1,", "start": 382.85, "duration": 4.65}, {"text": "sigma 2 down to sigma r there.", "start": 387.5, "duration": 3.0}, {"text": "The rank, which came way back in the first slides,", "start": 390.5, "duration": 4.32}, {"text": "tells you how many there are.", "start": 394.82, "duration": 2.01}, {"text": "Good, good.", "start": 396.83, "duration": 3.87}, {"text": "Oh, here's an example.", "start": 400.7, "duration": 2.53}, {"text": "So I took a small matrix because I'm", "start": 403.23, "duration": 2.51}, {"text": "doing this by pencil and paper and actually showing you", "start": 405.74, "duration": 3.69}, {"text": "that the singular value.", "start": 409.43, "duration": 3.16}, {"text": "So there is my matrix, 2 by 2.", "start": 412.59, "duration": 2.51}, {"text": "Here are the u's.", "start": 415.1, "duration": 1.21}, {"text": "Do you see that those are orthogonal--", "start": 416.31, "duration": 1.82}, {"text": "1, 3 against minus 3, 1?", "start": 418.13, "duration": 2.94}, {"text": "Take the dot product, and you get 0.", "start": 421.07, "duration": 2.08}, {"text": "The v's are orthogonal.", "start": 423.15, "duration": 1.81}, {"text": "The sigma is diagonal.", "start": 424.96, "duration": 1.84}, {"text": "And then the pieces from that add back to the matrix.", "start": 426.8, "duration": 5.09}, {"text": "So it's really, it's broken my matrix", "start": 431.89, "duration": 2.3}, {"text": "into a couple of pieces--", "start": 434.19, "duration": 2.37}, {"text": "one for the first singular value in vector,", "start": 436.56, "duration": 4.16}, {"text": "and the other for the second singular value in vector.", "start": 440.72, "duration": 3.26}, {"text": "And that's what data science wants.", "start": 443.98, "duration": 2.45}, {"text": "Data science wants to know what's important in the matrix?", "start": 446.43, "duration": 3.88}, {"text": "Well, what's important is sigma 1, the big guy.", "start": 450.31, "duration": 4.3}, {"text": "Sigma 2, you see.", "start": 454.61, "duration": 1.79}, {"text": "Well, it was 3 times smaller--", "start": 456.4, "duration": 1.62}, {"text": "3/2 versus 1/2.", "start": 458.02, "duration": 2.43}, {"text": "So if I had a 100 by 100 matrix or 100 by 1,000,", "start": 460.45, "duration": 5.31}, {"text": "I'd have 100 singular values and maybe the first five I'd keep.", "start": 465.76, "duration": 5.8}, {"text": "If I'm in the financial market, those guys,", "start": 471.56, "duration": 2.942}, {"text": "those first numbers are telling me", "start": 474.502, "duration": 2.988}, {"text": "maybe what bond prices are going to do over time.", "start": 477.49, "duration": 3.3}, {"text": "And it's a mixture of a few features,", "start": 480.79, "duration": 5.32}, {"text": "but not all 1,000 features, right.", "start": 486.11, "duration": 3.82}, {"text": "So this is singular value decomposition", "start": 489.93, "duration": 3.96}, {"text": "picks out the important part of a data matrix.", "start": 493.89, "duration": 3.56}, {"text": "And you cannot ask for a more than that.", "start": 497.45, "duration": 2.04}, {"text": "Here's what you do with a matrix is just totally enormous--", "start": 502.25, "duration": 3.45}, {"text": "too big to multiply--", "start": 505.7, "duration": 2.03}, {"text": "too big to compute.", "start": 507.73, "duration": 1.83}, {"text": "Then you randomly sample it.", "start": 509.56, "duration": 7.66}, {"text": "Yeah, maybe the next slide even mentions", "start": 517.22, "duration": 2.579}, {"text": "that word randomized numerical linear algebra.", "start": 519.799, "duration": 2.891}, {"text": "So this, I'll go back to this.", "start": 522.69, "duration": 3.48}, {"text": "So the singular value decomposition--", "start": 526.17, "duration": 2.81}, {"text": "this, what we just talked about with the u's and the v's", "start": 528.98, "duration": 3.43}, {"text": "and the sigmas.", "start": 532.41, "duration": 1.94}, {"text": "Sigma 1 is the biggest.", "start": 534.35, "duration": 2.32}, {"text": "Sigma r is the smallest.", "start": 536.67, "duration": 2.73}, {"text": "So in data science, you very often", "start": 539.4, "duration": 2.24}, {"text": "keep just these first ones, maybe the first k, the k", "start": 541.64, "duration": 4.77}, {"text": "largest ones.", "start": 546.41, "duration": 1.74}, {"text": "And then you've got the matrix that", "start": 548.15, "duration": 2.1}, {"text": "has rank only k, because you're only working with k vectors.", "start": 550.25, "duration": 4.65}, {"text": "And it turns out that's the closest one to the big matrix", "start": 554.9, "duration": 4.23}, {"text": "A. So this singular value is among other things", "start": 559.13, "duration": 4.48}, {"text": "is picking out, putting in order of importance", "start": 563.61, "duration": 4.32}, {"text": "the little pieces of the matrix.", "start": 567.93, "duration": 3.01}, {"text": "And then you can just pick a few pieces to work with.", "start": 570.94, "duration": 3.76}, {"text": "Yeah, yeah.", "start": 574.7, "duration": 1.39}, {"text": "And the idea of norms is how to measure the size of a matrix.", "start": 576.09, "duration": 4.53}, {"text": "Yeah, but I'll leave that for the future.", "start": 580.62, "duration": 5.44}, {"text": "And randomized linear algebra I just want to mention.", "start": 586.06, "duration": 4.17}, {"text": "Seems a little crazy that by just randomly sampling", "start": 590.23, "duration": 3.72}, {"text": "a matrix, we could learn anything about it.", "start": 593.95, "duration": 4.63}, {"text": "But typically data is sort of organized.", "start": 598.58, "duration": 3.98}, {"text": "It's not just totally random stuff.", "start": 602.56, "duration": 2.71}, {"text": "So if we want to know like, my friend in the Broad Institute", "start": 605.27, "duration": 4.55}, {"text": "was doing the ancient history of man.", "start": 609.82, "duration": 4.59}, {"text": "So data from thousands of years ago.", "start": 614.41, "duration": 4.06}, {"text": "So he had a giant matrix--", "start": 618.47, "duration": 1.62}, {"text": "a lot of data-- too much data.", "start": 620.09, "duration": 2.49}, {"text": "And he said, how can we find the singular value decomposition?", "start": 622.58, "duration": 5.3}, {"text": "Pick out the important thing.", "start": 627.88, "duration": 1.78}, {"text": "So you had to sample the data.", "start": 629.66, "duration": 3.55}, {"text": "Statistics is a beautiful important subject.", "start": 633.21, "duration": 3.78}, {"text": "And it leans on linear algebra.", "start": 636.99, "duration": 3.6}, {"text": "Data science leans on linear algebra.", "start": 640.59, "duration": 3.55}, {"text": "You are seeing the tool.", "start": 644.14, "duration": 2.51}, {"text": "Calculus would be functions would be continuous curves.", "start": 646.65, "duration": 5.43}, {"text": "Linear algebra is about vectors.", "start": 652.08, "duration": 2.655}, {"text": "This is just n components.", "start": 654.735, "duration": 2.565}, {"text": "And that's where you compute.", "start": 657.3, "duration": 1.71}, {"text": "And that's where you understand.", "start": 659.01, "duration": 2.34}, {"text": "OK.", "start": 661.35, "duration": 1.71}, {"text": "Oh, this is maybe the last slide to just", "start": 663.06, "duration": 4.06}, {"text": "help orient you in the courses.", "start": 667.12, "duration": 3.45}, {"text": "So at MIT 18.06 is the Linear Algebra Course.", "start": 670.57, "duration": 4.26}, {"text": "And maybe you know 18.06 and also", "start": 674.83, "duration": 3.12}, {"text": "18.06 Scholar, SC, on OpenCourseWare.", "start": 677.95, "duration": 5.44}, {"text": "And then this is the new course with the new book, 18.065.", "start": 683.39, "duration": 7.38}, {"text": "So its numbers sort of indicating a second course", "start": 690.77, "duration": 3.66}, {"text": "in linear algebra.", "start": 694.43, "duration": 0.75}, {"text": "That's when I'm actually teaching now,", "start": 695.18, "duration": 1.61}, {"text": "Monday, Wednesday, Friday.", "start": 696.79, "duration": 3.26}, {"text": "And so that starts with linear algebra,", "start": 700.05, "duration": 2.25}, {"text": "but it's mostly about deep learning--", "start": 702.3, "duration": 2.58}, {"text": "learning from data.", "start": 704.88, "duration": 1.08}, {"text": "So you need statistics.", "start": 705.96, "duration": 1.32}, {"text": "You need optimization, minimizing.", "start": 707.28, "duration": 3.24}, {"text": "Big functions of calculus comes into it.", "start": 710.52, "duration": 3.39}, {"text": "So that's a lot of fun to teach and to learn.", "start": 713.91, "duration": 4.26}, {"text": "And, of course, it's tremendously", "start": 718.17, "duration": 3.3}, {"text": "important in industry now.", "start": 721.47, "duration": 2.04}, {"text": "And Google and Facebook and ever so many companies", "start": 723.51, "duration": 3.06}, {"text": "need people who understand this.", "start": 726.57, "duration": 2.73}, {"text": "And, oh, and then I am repeating 18.06", "start": 729.3, "duration": 3.66}, {"text": "because there is this new book coming, I hope.", "start": 732.96, "duration": 3.21}, {"text": "Did some more this morning.", "start": 736.17, "duration": 2.97}, {"text": "Linear Algebra for Everyone.", "start": 739.14, "duration": 1.5}, {"text": "So I have optimistically put 2021.", "start": 740.64, "duration": 2.94}, {"text": "And you're the first people that know about it.", "start": 743.58, "duration": 3.56}, {"text": "So these are the websites for the two that we have.", "start": 747.14, "duration": 3.71}, {"text": "That's the website for the linear algebra", "start": 750.85, "duration": 1.95}, {"text": "book, math.mit.edu.", "start": 752.8, "duration": 2.59}, {"text": "And this is the website for the Learning from Data book.", "start": 755.39, "duration": 4.13}, {"text": "So you see there the table of contents and all and solutions", "start": 759.52, "duration": 4.47}, {"text": "to problems-- lots of things.", "start": 763.99, "duration": 3.84}, {"text": "Thanks for listening to this is--", "start": 767.83, "duration": 2.82}, {"text": "what-- maybe four or five pieces in this 2020 vision", "start": 770.65, "duration": 5.82}, {"text": "to update the videos that have been watched", "start": 776.47, "duration": 7.38}, {"text": "so much on OpenCourseWare.", "start": 783.85, "duration": 3.63}, {"text": "Thank you.", "start": 787.48, "duration": 1.85}]}